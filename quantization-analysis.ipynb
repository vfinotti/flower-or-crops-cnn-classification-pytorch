{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16000 images under train\n",
      "Loaded 2000 images under test\n",
      "Classes: \n",
      "['flower', 'sugarcane']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "## DATA LOADER\n",
    "data_dir = './images'\n",
    "TRAIN = 'train'\n",
    "VAL = 'val'\n",
    "TEST = 'test'\n",
    "\n",
    "# Squeezenet Takes 224x224 images as input, so we resize all of them\n",
    "data_transforms = {\n",
    "    TRAIN: transforms.Compose([\n",
    "        # Data augmentation is a good practice for the train set\n",
    "        # Here, we randomly crop the image to 224x224 and\n",
    "        # randomly flip it horizontally.\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    # VAL: transforms.Compose([\n",
    "    #     transforms.Resize(256),\n",
    "    #     transforms.CenterCrop(224),\n",
    "    #     transforms.ToTensor(),\n",
    "    # ]),\n",
    "    TEST: transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "    x: datasets.ImageFolder(\n",
    "        os.path.join(data_dir, x),\n",
    "        transform=data_transforms[x]\n",
    "    )\n",
    "    for x in [TRAIN, TEST]\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(\n",
    "        image_datasets[x], batch_size=8,\n",
    "        shuffle=True, num_workers=4\n",
    "    )\n",
    "    for x in [TRAIN, TEST]\n",
    "}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [TRAIN, TEST]}\n",
    "\n",
    "for x in [TRAIN, TEST]:\n",
    "    print(\"Loaded {} images under {}\".format(dataset_sizes[x], x))\n",
    "\n",
    "print(\"Classes: \")\n",
    "class_names = image_datasets[TRAIN].classes\n",
    "print(image_datasets[TRAIN].classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization classes and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is based on the code available at https://github.com/aaron-xichen/pytorch-playground. Although the refered repository offers some options for quantizing popular CNN architectures like Squeezenet, VGG, Alexnet and Resnet, a more dedicated code for my application was necessary in my opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "\n",
    "def compute_integral_part(input, overflow_rate):\n",
    "    \"\"\"Calculates the scaling factor (sf) that better represents the input\"\"\"\n",
    "    # transform 'input' into an array of the abs of each elements \n",
    "    abs_value = input.abs().view(-1)\n",
    "    # sort the modulus array in descending order\n",
    "    sorted_value = abs_value.sort(dim=0, descending=True)[0]\n",
    "    # find what index corresponds to the max possibe modulus value, considering the overflow_rate.\n",
    "    # for '0' overflow_rate, the index will be the one of the maximum module of all modules, and \n",
    "    # the biggest modulus (index 0) will be chosen\n",
    "    split_idx = int(overflow_rate * len(sorted_value))\n",
    "    # value at that index\n",
    "    v = sorted_value[split_idx]\n",
    "    #print('v is {}'.format(v))\n",
    "    if isinstance(v, Variable):\n",
    "        v = v.data.cpu().numpy()\n",
    "    # get the minimum ammount of bits required to represent the value chosen and consider it the \n",
    "    # scaling factor. The '1e-12' is there to determine the smallest precision (if 'v' is too small)\n",
    "    sf = math.ceil(math.log2(v+1e-12))\n",
    "    #print('sf is {}'.format(sf))\n",
    "    return sf\n",
    "\n",
    "def linear_quantize(input, sf, bits, return_type='float'):\n",
    "    \"\"\"Converts a float value from the real numbers domain to a float in the quantized domain\"\"\"\n",
    "    assert bits >= 1, bits\n",
    "    if bits == 1:\n",
    "        return torch.sign(input) - 1\n",
    "    #print('inside sf is {}'.format(sf))\n",
    "    # calculate the minimum step, considering that the 'sf' bits will quantize in the interval [0,1].\n",
    "    # this is equivalento to compute 1/(2^(sf)),  or 2^(-sf)\n",
    "    delta = math.pow(2.0, -sf)\n",
    "    bound = math.pow(2.0, bits-1)\n",
    "    # calculates min and maximum. For 8 bits, the quantized number will be between [-128,127]. \n",
    "    min_val = - bound\n",
    "    max_val = bound - 1\n",
    "    \n",
    "    # dividing the input by delta and flooring\n",
    "    ## rounded = torch.floor(input / delta + 0.5) # Equivalent to torch.round(input / delta)\n",
    "    rounded = torch.round(input / delta)\n",
    "    \n",
    "    # calculate the output format base on the return type desired\n",
    "    assert return_type in ['float', 'int'], 'Return type should be \\'float\\' or \\'int\\'!'\n",
    "    if return_type == 'float':\n",
    "        clipped_value = torch.clamp(rounded, min_val, max_val) * delta\n",
    "    elif return_type == 'int':\n",
    "        clipped_value = torch.clamp(rounded, min_val, max_val)\n",
    "    else:\n",
    "        # format not supported, returning float\n",
    "        clipped_value = torch.clamp(rounded, min_val, max_val) * delta\n",
    "    \n",
    "    return clipped_value\n",
    "\n",
    "\n",
    "class LinearQuant(nn.Module):\n",
    "    def __init__(self, name, bits, sf=None, overflow_rate=0.0, counter=10):\n",
    "        super(LinearQuant, self).__init__()\n",
    "        self.name = name\n",
    "        self._counter = counter\n",
    "\n",
    "        self.bits = bits\n",
    "        self.sf = sf\n",
    "        self.overflow_rate = overflow_rate\n",
    "\n",
    "    @property\n",
    "    def counter(self):\n",
    "        return self._counter\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self._counter > 0:\n",
    "            self._counter -= 1\n",
    "            sf_new = self.bits - 1 - compute_integral_part(input, self.overflow_rate)\n",
    "            self.sf = min(self.sf, sf_new) if self.sf is not None else sf_new\n",
    "            return input\n",
    "        else:\n",
    "            output = linear_quantize(input, self.sf, self.bits)\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(sf={}, bits={}, overflow_rate={:.3f}, counter={})'.format(\n",
    "self.__class__.__name__, self.sf, self.bits, self.overflow_rate, self.counter)\n",
    "\n",
    "\n",
    "def duplicate_model_with_quant(model, bits, overflow_rate=0.0, counter=10):\n",
    "    \"\"\"assume that original model has at least a nn.Sequential\"\"\"\n",
    "    \n",
    "    import squeezenet_fix\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    \n",
    "    # Quantized Fire class\n",
    "    class Fire_Quant(nn.Module):\n",
    "        def __init__(self, inplanes, squeeze_planes,\n",
    "                     expand1x1_planes, expand3x3_planes):\n",
    "            super(Fire_Quant, self).__init__()\n",
    "            self.inplanes = inplanes\n",
    "            self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
    "            self.squeeze_quant = LinearQuant('squeeze_quant', bits=32)\n",
    "            self.squeeze_activation = nn.ReLU(inplace=True)\n",
    "            self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
    "                                       kernel_size=1)\n",
    "            self.expand1x1_quant = LinearQuant('expand1x1_quant', bits=32)\n",
    "            self.expand1x1_activation = nn.ReLU(inplace=True)\n",
    "            self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
    "                                       kernel_size=3, padding=1)\n",
    "            self.expand3x3_quant = LinearQuant('expand3x3_quant', bits=32)\n",
    "            self.expand3x3_activation = nn.ReLU(inplace=True)\n",
    "        def forward(self, x):\n",
    "            x = self.squeeze_activation(self.squeeze_quant(self.squeeze(x)))\n",
    "            return torch.cat([\n",
    "                self.expand1x1_activation(self.expand1x1_quant(self.expand1x1(x))),\n",
    "                self.expand3x3_activation(self.expand3x3_quant(self.expand3x3(x)))\n",
    "            ], 1)\n",
    "\n",
    "\n",
    "    \n",
    "    if isinstance(model, nn.Sequential):\n",
    "        l = OrderedDict()\n",
    "        for k, v in model._modules.items():\n",
    "            if isinstance(v, (nn.Conv2d, nn.Linear, nn.BatchNorm1d, nn.BatchNorm2d, nn.AvgPool2d)):\n",
    "                l[k] = v\n",
    "                quant_layer = LinearQuant('{}_quant'.format(k), bits=bits, overflow_rate=overflow_rate, counter=counter)\n",
    "                l['{}_{}_quant'.format(k, v.__class__.__name__)] = quant_layer\n",
    "            else:\n",
    "                l[k] = duplicate_model_with_quant(v, bits, overflow_rate, counter)\n",
    "        m = nn.Sequential(l)\n",
    "        return m\n",
    "\n",
    "    elif isinstance(model, squeezenet_fix.Fire):\n",
    "        inplanes = model.inplanes\n",
    "        squeeze_planes = model.squeeze.out_channels\n",
    "        expand1x1_planes = model.expand1x1.out_channels\n",
    "        expand3x3_planes = model.expand3x3.out_channels\n",
    "        \n",
    "        m = Fire_Quant(inplanes, squeeze_planes, expand1x1_planes, expand3x3_planes)\n",
    "\n",
    "        # copy layers (with weights) to the quantized layer\n",
    "        m.squeeze = model.squeeze\n",
    "        m.expand1x1 = model.expand1x1\n",
    "        m.expand3x3 = model.expand3x3\n",
    "       \n",
    "        # adjust the LinearQuant layers\n",
    "        m.squeeze_quant = LinearQuant('squeeze_quant', bits=bits, overflow_rate=overflow_rate, counter=counter)\n",
    "        m.expand1x1_quant = LinearQuant('expand1x1_quant', bits=bits, overflow_rate=overflow_rate, counter=counter)\n",
    "        m.expand3x3_quant = LinearQuant('expand3x3_quant', bits=bits, overflow_rate=overflow_rate, counter=counter)\n",
    "        \n",
    "        return m\n",
    "    else:\n",
    "        for k, v in model._modules.items():\n",
    "            model._modules[k] = duplicate_model_with_quant(v, bits, overflow_rate, counter)\n",
    "    return model\n",
    "\n",
    "def eval_model(squeezenet, criterion, verbose=False):\n",
    "    since = time.time()\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    loss_test = 0\n",
    "    acc_test = 0\n",
    "\n",
    "    test_batches = len(dataloaders[TEST])\n",
    "    if verbose:\n",
    "        print(\"Evaluating model\")\n",
    "        print('-' * 10)\n",
    "\n",
    "    squeezenet.train(False)\n",
    "    squeezenet.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders[TEST]):\n",
    "            if verbose:\n",
    "                if i % 10 == 0:\n",
    "                    print(\"\\rTest batch {}/{}\".format(i, test_batches), end='', flush=True)\n",
    "\n",
    "\n",
    "            inputs, labels = data\n",
    "            if use_gpu:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            else:\n",
    "                inputs, labels = inputs, labels\n",
    "\n",
    "            outputs = squeezenet(inputs)\n",
    "\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss_test += loss.data\n",
    "            acc_test += torch.sum(preds == labels.data).item()\n",
    "\n",
    "            # del inputs, labels, outputs, preds\n",
    "            # torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = loss_test / dataset_sizes[TEST]\n",
    "    avg_acc = acc_test / dataset_sizes[TEST]\n",
    "\n",
    "    elapsed_time = time.time() - since\n",
    "    if verbose:\n",
    "        print()\n",
    "        print(\"Evaluation completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n",
    "        print(\"Avg loss (test): {:.4f}\".format(avg_loss))\n",
    "        print(\"Avg acc (test): {:.4f}\".format(avg_acc))\n",
    "        print('-' * 10)\n",
    "    return avg_acc, avg_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.6000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "0\n",
      "v is 5\n",
      "2.321928094887651\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, 3)\n",
    "x[0,2] = 1\n",
    "x[0,2] = 3.6\n",
    "abs_values = x.abs().view(-1)\n",
    "sorted_values = abs_values.sort(dim=0, descending=True)[0]\n",
    "print(sorted_values)\n",
    "\n",
    "overflow_rate = 0.0\n",
    "split_idx = int(overflow_rate * len(sorted_values))\n",
    "print(split_idx)\n",
    "\n",
    "v = sorted_values[split_idx]\n",
    "v = 5\n",
    "print(\"v is {}\".format(v))\n",
    "a = math.log2(v+1e-12)\n",
    "print(a)\n",
    "\n",
    "sf = compute_integral_part(x, 0.0)\n",
    "print(sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sf is 5\n",
      "post-calculated sf is 2.0\n",
      "delta is 0.25\n",
      "a ([32.]) quantized is [128.]\n",
      "a ([32.]) rounded is [32.]\n"
     ]
    }
   ],
   "source": [
    "bits = 8\n",
    "\n",
    "x = torch.ones(1, 1)\n",
    "x[0,0] = 25\n",
    "\n",
    "sf = compute_integral_part(x, 0.0)\n",
    "print('sf is {}'.format(sf))\n",
    "\n",
    "sf = bits - 1. - sf\n",
    "print('post-calculated sf is {}'.format(sf))\n",
    "\n",
    "delta = math.pow(2.0, -sf)\n",
    "print('delta is {}'.format(delta))\n",
    "\n",
    "a = torch.Tensor(1)\n",
    "a[0] = 32\n",
    "quantized = torch.round(a / delta)\n",
    "print('a ({}) quantized is {}'.format(a.numpy(), quantized.numpy()))\n",
    "rounded = quantized*delta\n",
    "print('a ({}) rounded is {}'.format(a.numpy(), rounded.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark =True\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "#parser = argparse.ArgumentParser(description='PyTorch SVHN Example')\n",
    "# parser.add_argument('--type', default='cifar10', help='|'.join(selector.known_models))\n",
    "# parser.add_argument('--quant_method', default='linear', help='linear|minmax|log|tanh')\n",
    "#parser.add_argument('--batch_size', type=int, default=100, help='input batch size for training (default: 64)')\n",
    "#parser.add_argument('--gpu', default=None, help='index of gpus to use')\n",
    "#parser.add_argument('--ngpu', type=int, default=8, help='number of gpus to use')\n",
    "#parser.add_argument('--seed', type=int, default=117, help='random seed (default: 1)')\n",
    "# parser.add_argument('--model_root', default='~/.torch/models/', help='folder to save the model')\n",
    "# parser.add_argument('--data_root', default='/tmp/public_dataset/pytorch/', help='folder to save the model')\n",
    "# parser.add_argument('--logdir', default='log/default', help='folder to save to the log')\n",
    "\n",
    "#parser.add_argument('--input_size', type=int, default=224, help='input size of image')\n",
    "# parser.add_argument('--n_sample', type=int, default=20, help='number of samples to infer the scaling factor')\n",
    "#parser.add_argument('--param_bits', type=int, default=8, help='bit-width for parameters')\n",
    "#parser.add_argument('--bn_bits', type=int, default=32, help='bit-width for running mean and std')\n",
    "#parser.add_argument('--fwd_bits', type=int, default=8, help='bit-width for layer output')\n",
    "# parser.add_argument('--overflow_rate', type=float, default=0.0, help='overflow rate')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "args = {}\n",
    "\n",
    "args['batch_size'] = 100\n",
    "args['gpu'] = None\n",
    "args['ngpu'] = 8\n",
    "args['seed'] = 117\n",
    "args['input_size'] = 224\n",
    "#args['param_bits'] = 2\n",
    "#args['bn_bits'] = 2\n",
    "#args['fwd_bits'] = 2\n",
    "args['overflow_rate'] = 0.0\n",
    "\n",
    "#args.gpu = misc.auto_select_gpu(utility_bound=0, num_gpu=args.ngpu, selected_gpus=args.gpu)\n",
    "#args.ngpu = len(args.gpu)\n",
    "#misc.ensure_dir(args.logdir)\n",
    "#args.model_root = misc.expand_user(args.model_root)\n",
    "#args.data_root = misc.expand_user(args.data_root)\n",
    "#args.input_size = 299 if 'inception' in args.type else args.input_size\n",
    "#assert args.quant_method in ['linear', 'minmax', 'log', 'tanh']\n",
    "#print(\"=================FLAGS==================\")\n",
    "#for k, v in args.__dict__.items():\n",
    "#    print('{}: {}'.format(k, v))\n",
    "#print(\"========================================\")\n",
    "\n",
    "#assert torch.cuda.is_available(), 'no cuda'\n",
    "torch.manual_seed(args['seed'])\n",
    "torch.cuda.manual_seed(args['seed'])\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu:\n",
    "    print(\"Using CUDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model():\n",
    "    # load model and dataset fetcher\n",
    "    # model_raw, ds_fetcher, is_imagenet = selector.select(args.type, model_root=args.model_root)\n",
    "    #squeezenet1_1 = models.squeezenet1_1()\n",
    "\n",
    "    # importing fixed version of squeezenet class and functions\n",
    "    import squeezenet_fix\n",
    "\n",
    "    squeezenet1_1 = squeezenet_fix.squeezenet1_1()\n",
    "\n",
    "    # Freeze training for all layers\n",
    "    for param in squeezenet1_1.features.parameters():\n",
    "        param.require_grad = False\n",
    "\n",
    "    # Newly created modules have require_grad=True by default\n",
    "    num_features = squeezenet1_1.classifier[1].in_channels\n",
    "    features = list(squeezenet1_1.classifier.children())[:-3] # Remove last 3 layers\n",
    "    features.extend([nn.Conv2d(num_features, 2, kernel_size=1)]) # Add\n",
    "    features.extend([nn.ReLU(inplace=True)]) # Add\n",
    "    features.extend([nn.AdaptiveAvgPool2d(output_size=(1,1))]) # Add our layer with 2 outputs\n",
    "    squeezenet1_1.classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "\n",
    "    if use_gpu:\n",
    "        squeezenet1_1.load_state_dict(torch.load('./weights/squeezenet_v1-flower-or-crops.pt'))\n",
    "    else:\n",
    "        squeezenet1_1.load_state_dict(torch.load('./weights/squeezenet_v1-flower-or-crops.pt', map_location='cpu'))\n",
    "    #print(squeezenet1_1)\n",
    "    return squeezenet1_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_state_dict(state_dict, bn_bits, param_bits, return_type='float'):\n",
    "    if param_bits < 32:\n",
    "        state_dict_quant = OrderedDict()\n",
    "        sf_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            if 'running' in k:\n",
    "                if bn_bits >=32:\n",
    "                    print(\"Ignoring {}\".format(k))\n",
    "                    state_dict_quant[k] = v\n",
    "                    continue\n",
    "                else:\n",
    "                    bits = bn_bits\n",
    "            else:\n",
    "                bits = param_bits\n",
    "\n",
    "    #        if args.quant_method == 'linear':\n",
    "    #            sf = bits - 1. - quant.compute_integral_part(v, overflow_rate=args.overflow_rate)\n",
    "    #            v_quant  = quant.linear_quantize(v, sf, bits=bits)\n",
    "    #        elif args.quant_method == 'log':\n",
    "    #            v_quant = quant.log_minmax_quantize(v, bits=bits)\n",
    "    #        elif args.quant_method == 'minmax':\n",
    "    #            v_quant = quant.min_max_quantize(v, bits=bits)\n",
    "    #        else:\n",
    "    #            v_quant = quant.tanh_quantize(v, bits=bits)\n",
    "    \n",
    "            # The sf will be used to do the quantization. Subtract 1 for dividind the range by 2\n",
    "            # (2^(-sf) will be calculated after), so half of the quatized range represents positive\n",
    "            # numbers and the other half negative numbers. Subtract the ammount of bits required to\n",
    "            # represent the max abs value of the input to adjust the scale. At the end of the day, \n",
    "            # the operation done through these steps is equivalent to consider that you have \"bits - 1\"\n",
    "            # bits to quantize the maximum modulus of the input array. I don't know why to make such simple\n",
    "            # operation not explicit...\n",
    "            sf = bits - 1. - compute_integral_part(v, overflow_rate=args['overflow_rate'])\n",
    "            #sf = compute_integral_part(v, overflow_rate=args['overflow_rate'])\n",
    "            v_quant  = linear_quantize(v, sf, bits=bits, return_type=return_type)     \n",
    "            state_dict_quant[k] = v_quant\n",
    "        return state_dict_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "\n",
    "\n",
    "def quantize_model_forward_activation(model, fwd_bits):\n",
    "    # Quantize the forward activaton of parameters on the model\n",
    "    if fwd_bits < 32:\n",
    "        model = duplicate_model_with_quant(model, bits=fwd_bits)\n",
    "        #print(squeezenet1_1)\n",
    "        #val_ds_tmp = ds_fetcher(10, data_root=args.data_root, train=False, input_size=args.input_size)\n",
    "        #misc.eval_model(model_raw, val_ds_tmp, ngpu=1, n_sample=args.n_sample, is_imagenet=is_imagenet)\n",
    "        if use_gpu:\n",
    "            model.cuda() #.cuda() will move everything to the GPU side\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse quantization vs accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse the accuracy on the test set over different levels of quantization. With this information, it is possible to choose the most suitable quantization level for the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Tests 31/31"
     ]
    }
   ],
   "source": [
    "bits_list = list(range(1, 32))\n",
    "#bits_list = list(range(8, 9))\n",
    "\n",
    "test_accuracies_list = []\n",
    "test_losses_list = []\n",
    "\n",
    "for idx, bits_item in enumerate(bits_list):\n",
    "    print(\"\\rRunning Tests {}/{}\".format(idx+1, len(bits_list)), end='', flush=True)\n",
    "    # Quantize weights\n",
    "    squeezenet1_1 = create_model()\n",
    "    state_dict = squeezenet1_1.state_dict()\n",
    "    state_dict_quant = quantize_state_dict(state_dict, bits_item, bits_item)\n",
    "    #print(state_dict_quant)\n",
    "    squeezenet1_1.load_state_dict(state_dict_quant)\n",
    "\n",
    "    # Quantize forward activation\n",
    "    squeezenet1_1_quant = quantize_model_forward_activation(squeezenet1_1, bits_item)\n",
    "    #print()\n",
    "    #print(squeezenet1_1_quant)\n",
    "\n",
    "    # evaluate\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    #exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "    avg_acc, avg_loss = eval_model(squeezenet1_1_quant, criterion)\n",
    "    test_accuracies_list.append(avg_acc)\n",
    "    test_losses_list.append(avg_loss)\n",
    "    \n",
    "    #print()\n",
    "    #print(idx)\n",
    "    #print(bits_item)\n",
    "    #print(avg_acc)\n",
    "    #print(test_accuracies_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3de3wddZ3/8dcnlzZpkya9EUpb2goFW8rNVvDCJUVw666KuK6CIqCLlV1ZxZ+6oHsRdXdl1yu7sLIFEUWhoqAgYgGVWBAVKBRoKaWVltL7heY0aZPmJPn8/phJenJ6kp62mTNzct7PxyOPzO3MfL4zc+Zz5jsz3zF3R0RESldZ3AGIiEi8lAhEREqcEoGISIlTIhARKXFKBCIiJU6JQESkxCkRSOJY4HtmttPMnohoGWvN7Nyw+wtmdkvGuAvM7FUzazWzU83seDN7xsxazOyTUcQzVGSvy0Gc701m9i8RzPdaM/vhYM83axmNZrY+ymUcroq4A0gyM2sCTgaOdPe9MYdTSs4AzgMmufvuqBfm7v+RNejrwJXufi+AmX0XaHL3U6OOJZuZNQI/dPdJhV72geSKLce6PJT5XgZc7u5nZMz3isOdr/RPZwT9MLOpwJmAA+8u8LKLKkFHEO8UYO2hJIFBimUKsHyA/kLHIxItd9dfjj/gX4HfA98E7s8aVw18A3gFSAGPAdXhuDOAx4Fm4FXgsnB4E8GvnJ55XAY8ltHvwCeAVcCacNj14Tx2AUuAMzOmLwe+APwZaAnHTwZuBL6RFe8vgKv6KecJwMPAa8AW4Avh8NuAf8uYrhFYn9G/FrgaeA7YC/wz8NOseV8P/HfYXQd8F9gEbAD+DSjPEc/fAu1AF9AKfCkc/jFgdRjnfcBRA627HPP9cLi9dgD/FMZ/bjjuWuCHwPBwmQ7sDtftb8NY2sNxx4XTfR1YF66zmzK2fyOwPlw3m4Hbw+HvBJaG+8XjwElZ6/Kz4bpMAT8GqoCRQBvQHS67NbPcGZ8fG66TXcATwFcI9y1galieiozpmwj3ReCYsIw7gO3Aj4D6Q42tZ12Gn70hY1wr0AlcG467hn377gvABeHwGVnbv7mf/fFA+8MVBPvDToLvhPWzX/TGG/a/iX3f32eBxnD4hcBTWZ/9NHBf2H3AfSLjc1cTfAdagJXA22I/3sUdQFL/wp3s74HZQBpoyBh3Y/hlmkhwQH5LuCMcHW7ci4DK8At6SviZ3i9f2H8Z+yeCh4ExGTvQxeE8KoDPEBxYqsJxnwOeB44HjKAKayxwGrARKAunGwfsyYw/Y5m1BAfmzxB8uWuB08Nx2V+87J15LcGBbTJBYpwSLmdUOL48nPebwv6fA/9HcAA5guCA9fF+1n32ujmH4CD1hnA9/w+weKB1lzW/mQQHlbPCz3+T4KDUJxFkze/YjP7sbfdtgoPPmHCd/QL4asZ66gT+M1xWdRj3VuD0cL1cGq6/4Rnr8gmCA+kYYAVwRa713s/6WgjcFa7bWQQHmXwTwbEE1XDDgfHAYuDbWds579iy12XG8FOAbcCpYf/fhPMsAz5AkHgn5Nr+2ftjnvvD/UA9wXdyGzCvn3XXGy/B93kH8JdhXOeF/eOBEQTf7ekZn30SuDDPfWJ92H08wY+7ozK2zzGxH+/iDiCJfwS/6tPAuLD/ReDTYXcZwS+hk3N87vPAz/qZZ++XL+zvs7OHO+85B4hrZ89yCX5JnN/PdCuA88LuK4EH+pnuIuCZfsb1fvHC/j5feoIDxEezPvMYcEnYfR7w57C7geCsoTpr2Y/0s+zsdfNd4L8y+mvC7TM1n3VHcHa3MKN/JNDBISQCgqS7O/PLC7yZfWdxjeG8qzLGfwf4SlZMK4GzM9blxRnj/gu4Kdd6z1G28nBdvD5j2H+QZyLIMb/3ZO4TBxtb9roMh40P53PhAOVYSrg/Z2//7P0xz/3hjIzxdwHX9LPc3ngJfqnfnjX+QeDSsPuHwL+G3dMJEsOIPPeJnkRwLMGPgnOByv7WR6H/dI0gt0uBh9x9e9h/RzgMgl/YVQSntdkm9zM8X69m9pjZZ8xshZmlzKyZoHplXB7L+j7B2QTh/9v7mW5Q4yVYTxeF3R8M+yE4W6gENplZc1iW/yM4M8jHUQTVOgC4eyvBL7WJA8SS/fne8R5ce9iR57Kz9fw6XJJRlkXh8B7b3L09o38K8Jme6cPPTA7j6rE5o3sPwcEt33gq6Fv+V/qZdj9mdoSZLTSzDWa2i+BgNy5rskONDTOrBH4K3OHuCzOGX2JmSzPWx6wcy+1PPvvDocQ8BfibrO10BjAhHJ+9f//c3feQ3z7RE+tq4CqCBLQ1XPdHZU9XaEoEWcysGng/cLaZbTazzQR1gSeb2ckEp6TtBHWr2V7tZzgEvxhGZPQfmWMaz4jjTIJfKO8HRrt7PUEdreWxrB8C54fxziColsll0OIN/QRoNLNJwAXsSwSvEpwRjHP3+vBvlLuf0M+ys20k+JICYGYjCarBNgwQS6ZNBAfens+PCD9/KLYTnBGekFGWOnfPPNBkx/Iq8O8Z09e7+wh3vzOP5Q1ULgiqPTrJKB9BdUiPngvu/W3Lr4bLOMndRxH8cDDyc6DYIKi2aSG4hgSAmU0BbiY4Wx0b7tvLMpZ7oPnmsz8cilcJzggyt9NId78uHP8QMM7MTiFICD37dz77RC93v8ODO6KmEJT1Pw8z7sOmRLC/9xBcqJpJUK95CsHB9FGCao9u4Fbgm2Z2lJmVm9mbzWw4wYW2c83s/WZWYWZjw50GglPf95rZCDM7luCi6EBqCb7g24AKM/tXYFTG+FuAr5jZ9PC++5PMbCyAu68nqL+8Hbjb3dv6Wcb9wJFmdpWZDTezWjM7PSPevzSzMWZ2JMGvmAG5+zaCaofvEZwWrwiHbyL4En3DzEaZWZmZHWNmZx9onqE7gI+Y2Snhev4P4E/uvjbPz/8UeKeZnWFmw4Avc4j7frj9bwa+ZWZHAJjZRDP7iwE+djNwhZmdHm6rkWb2V2ZWm8citwBjzayun3i6gHuAa8N9ayb7zl57tskG4OJwX/0ofZN/LeFFWTObSHDtKV8DxmZmHwfOBj4YrrceIwkOgNvC6T5CcEaQOd9J4bbK5XD3h/78EHiXmf1FuK6qwmcAJgG4eyfBvvQ1gmsBD4fD894nwmdSzgnjbidIIF2HGfdhUyLY36XA99x9nbtv7vkjuAPiQ+HtgJ8luFD7JMFdC/9JcHF2HcGFps+Ew5cSXMQF+BZB3fEWgqqbHx0gjgeBXwEvEZwGt9P39P+bBHWfDxHcLfJdgguTPb4PnEj/1UK4ewtBXf67CE6lVwFzw9G3E9w1sTZcxo8PEG+POwjqP+/IGn4JMIzgDpGdBF+oCeTB3X8D/AtwN8Gv+2MI7uLIi7svJ7ir6I7w8zsJ7uw5VFcT3Ezwx7A65dcEFwH7W/5TBHe53BAuezVBPXg+sb8I3Am8HFY75KpGuJKg6mMzQV3697LGf4zgAL+D4C6xxzPGfYngomsK+CVBUslLHrFdBLwO2GjBw3mtZvYFd3+B4K67PxB8H04kuEOvx28JbtfdbGbbs+Z52PvDAOV5FTif4G68bQTft8/R9zjZs3//JEwMPfLdJ4YD1xGcRWwmqB79wuHGfrgsvIAhQ4yZnUXwC2dq1q8xGeJyPZAlMhCdEQxB4QW6TwG3KAmIyIEoEQwxZjaD4GGYCQT3NouIDEhVQyIiJU5nBCIiJa7oGsQaN26cT506tc+w3bt3M3LkyHgCGmQqS/IMlXKAypJUhSjLkiVLtrv7fg+5AdE1MUFwr/1WYFk/4w34b4Jbrp4D3pDPfGfPnu3ZHnnkkf2GFSuVJXmGSjncVZakKkRZyGo0L/Mvyqqh24B5A4x/B0F7HdOB+QTtsYiISIFFlgjcfTHBQ1X9OR/4QZis/gjUm1leDxiJiMjgifSuofDlLve7+6wc4+4HrnP3x8L+3wBXe/AUZva08wnOGmhoaJi9cOHCPuNbW1upqcm7HaxEU1mSZ6iUA1SWpCpEWebOnbvE3efkGhfnxeJcDVvlzEruvgBYADBnzhxvbGzsM76pqYnsYcVKZUmeoVIOUFmSKu6yxHn76Hr6tpg4iaBVQRERKaA4zwjuA640s4UEb25KedBKpQyCFZtSLFq2hQ3NbUysr2berAZmTMjZSGRs8o2xGMoiUswiSwRmdifBm3nGmdl64IsELyfB3W8CHiBoqXM1wYsjPhJVLKVmxaYUCxavoa66kgl1VaTa0ixYvIb5Z03b7wB6MAfZgz1wL3lxL8+kX8o5Xb4xRlGWwSxH1MsudFmiSM5DpSxRrpvBKsuhKromJubMmeNPPdX3enLc9WuDaTDK8q2HXyLVlqar2/nztlYqyozubqduxDA+dPrRjKqupLaqgs2pdu58Yh31I4ZRW1VBS3snqbZ070HW3ensdjq7nOUbm/ne79dSM7yCkcMr2L23k5b2Tj50+tFMb9jXrP6qLS386E/rqK2q4LWtmxg1roFUWxfvfcNRHD1mJB1d3XR2OQufWEdLe5rqYeX07IJ7OjoZMayC80/Z96Kpe5du6B1uBmVmtKW7GFVVyQdPP5rK8jIqyo11O/bw0yXrqauuoLaqMq/48i3Hzq2bGH3EhJzTHeo845puoLIM9nKHUlmiXjf5lqV+RCV7O73P9zRfZtbvxWIlgoQZjLJ89ifPsq2lncdWbacrj81bUWZUlpfhOF1d3XQDhtHZXVz7hshQN3vKaM44dhyptjR11ZV8+rzj8v7sQImg6JqYkIFtbWnnybU7eGVHG1PGjODcmQ1UlhnbW/cyrKKMd58ykV1taVraO7n9j2sZXlFGR6eT7uqmzAwzpy3dzbkzGqgoL6OyzKgoL+NXyzZRX11BeVkZFt7v5e6k2jr569mTepd/d/ir3Mx4bccOjhg/HsNJtXfy0bdOo7KijMqyMn72zPreX/Y9t4+17u2kZngFHzy99y2E3PGnV3qHdwPd3d57JvHukyeS7uqms9u57fG11FVV0O3g4c1nB4qvx4Gm27F9B2PHjc053aHOM67pBirLYC93KJUl6nWTb1nG1wwHoLaqgg3N/b148OApEQwhDy3fzDX3PM+utg5OnlTHqUfXM2JYOS3tnXQ5fPjNU/qcSq57bU/vL4se/f3SaE939TvthzIO3Ft37e0dvjbdzNQJo0i1pXl9dSVvP2Hfq3LH1lSyYPEaqirLe6ul2ju7ueQtfWO8pHwKCxavYXjGdJ3dvt9p8crNLQcd30GVY2J9zukOdZ5xTTdQWQZ7uUOpLFGvm4MtS0t7JxPrqxksan10CNi9t5Nr7n6O+bcvYUJdFYuuOovr/vpE6qqHsSnVTl11Zc76xHmzGki1pUm1pel27+2eN6thv2XkO23mdD7AdDMm1DH/rGnUVVcOGGO+0x1KfINRjqiXXciyDPZyh1JZol43g1GWw6FrBAlzoLJk3z0wdWw11/9mNa+8tocrzj6GT597HMMq8s/v0d41tIbZr59WsNs9o7s75cDlSPodKvmWpbjuGipsWaK9a2hwyjIQXSwuIgOVJfNWypHDy/n9qu0sXZ9ifM0wbvjgGzj9dWMLG+wBDJXtMlTKASpLUhWiLLpYPEQsWraFuupKyiy4gLR5115eN24kbz+hIXFJQESKhxJBEdnQ3BZcA1i2mR2tHcybdSTTj6hhU6o97tBEpIjpYnERmVhfTaotzSs7dnPckbUc31A76HcPiEjpUSIoIvNmNbB2+246upwpY6ojuXtAREqPqoaKyIwJdRw5qgoDKivKqauu5ANvnKQG2ETksCgRFJnlm3bxxmljuP7CU+MORUSGCFUNFZGtLe0s37iLxuPHxx2KiAwhSgRF5HcrtwHQeNwRMUciIkOJEkERaXppG0fUDmfGhNoDTywikiclgiLR2dXNY6u2c/Zx4/u0bCgicriUCIrEs+ubSbWlaTxe1UIiMriUCIpE08ptlJcZZ0wfF3coIjLEKBEUiaaV2zh1cn2fNslFRAaDEkER2N66l+c3pHTbqIhEQomgCCx+KbxtVNcHRCQCSgRFoGnlNsbVDGfmhFFxhyIiQ5ASQcJ1dTuLV23jrOPGUVam20ZFZPApESTcs+ubad6j20ZFJDpKBAn3u5XbKDM4S7eNikhElAgSrumlbZwyuZ76EcPiDkVEhiglggTb0bqX59Y3c7YamRORCCkRJNijq7bjjp4fEJFIKREk2O9e2sbYkcM4caLeQCYi0VEiSKjubmfxS9s467jxum1URCKlRJBQz29IsWN3B2cfp2ohEYmWEkFCNa3chhmcpUQgIhGLNBGY2TwzW2lmq83smhzjR5vZz8zsOTN7wsxmRRlPMWl6aSsnTapnzEjdNioi0YosEZhZOXAj8A5gJnCRmc3MmuwLwFJ3Pwm4BLg+qniKyc7dHTz7ajONOhsQkQKI8ozgNGC1u7/s7h3AQuD8rGlmAr8BcPcXgalm1hBhTEXh0dXb6XY4W7eNikgBVEQ474nAqxn964HTs6Z5Fngv8JiZnQZMASYBWzInMrP5wHyAhoYGmpqa+syktbV1v2HFqrW1lR8/9zw1ldD856U0vVy8dwwNle0yVMoBKktSxV2WKBNBriOYZ/VfB1xvZkuB54FngM79PuS+AFgAMGfOHG9sbOwzvqmpiexhxeq3jzzCyl2dnDPzKM6Ze2rc4RyWobJdhko5QGVJqrjLEmUiWA9MzuifBGzMnMDddwEfATAzA9aEfyVr3a5utrd26GliESmYKK8RPAlMN7NpZjYMuBC4L3MCM6sPxwFcDiwOk0PJem57FwBnTlciEJHCiOyMwN07zexK4EGgHLjV3Zeb2RXh+JuAGcAPzKwLeAH426jiKRbPb+vixIl1jK8dHncoIlIioqwawt0fAB7IGnZTRvcfgOlRxlAsVmxKce8zG1nV3M1p9WWs2JRixgS1MSQi0dOTxQmwYlOKBYvXsGJzUCs2rnZ40L8pFXNkIlIKlAgSYNGyLdRVV7KtZS8VBseMr6GuupJFy7Yc+MMiIodJiSABNjS3UVtVwY7dHYyugjIzaqsq2NDcFndoIlIClAgSYGJ9NS3tnexNd1MZNjnd0t7JxPrqmCMTkVKgRJAA82Y1kGpLsyfdSWWZk2pLk2pLM29Wybe2ISIFoESQADMm1PGxM6eR7nS63KirrmT+WdN015CIFESkt49K/iaPGYEDb51YyafPOy7ucESkhOiMICFSbWkARlTGHIiIlBwlgoToSQQjK4q3tVERKU5KBAmR2hMmgkolAhEpLCWChOg9I1DVkIgUmBJBQuxLBDojEJHCUiJIiH0Xi5UIRKSwlAgSItWWpqLMqCqPOxIRKTVKBAnR3JamrrqS4EVtIiKFo0SQEKkwEYiIFJoSQULsakszSolARGKgRJAQqbY09XqsWERioESQEM17VDUkIvFQIkgIXSMQkbgoESRAd7ezq12JQETioUSQAC17O3FHiUBEYqFEkAA9Dc4pEYhIHJQIEqCneQklAhGJgxJBAigRiEiclAgSoDcR6DkCEYmBEkEC9CSC+uphMUciIqVIiSABmts6AFUNiUg8lAgSINWWZlh5GVWV2hwiUng68iRAT4NzaoJaROKgRJAAanBOROIUaSIws3lmttLMVpvZNTnG15nZL8zsWTNbbmYfiTKepFKDcyISp8gSgZmVAzcC7wBmAheZ2cysyT4BvODuJwONwDfMrORunVGDcyISpyjPCE4DVrv7y+7eASwEzs+axoFaCyrHa4DXgM4IY0okJQIRiVOUiWAi8GpG//pwWKYbgBnARuB54FPu3h1hTImkRCAicaqIcN65boHxrP6/AJYC5wDHAA+b2aPuvqvPjMzmA/MBGhoaaGpq6jOT1tbW/YYVi253Wto72bllA01N24q6LNmGSlmGSjlAZUmq2Mvi7pH8AW8GHszo/zzw+axpfgmcmdH/W+C0geY7e/Zsz/bII4/sN6xYvNa616dcfb9/99GX3b24y5JtqJRlqJTDXWVJqkKUBXjK+zmuRlk19CQw3cymhReALwTuy5pmHfA2ADNrAI4HXo4wpsRRg3MiEre8EoGZ3W1mf2VmeScOd+8ErgQeBFYAd7n7cjO7wsyuCCf7CvAWM3se+A1wtbtvP7giFDclAhGJW77XCL4DfAT4bzP7CXCbu794oA+5+wPAA1nDbsro3gi8Pf9wh57eBuf0QJmIxCSvX/ju/mt3/xDwBmAtwUXdx83sI2amI9hhaNYZgYjELO+qHjMbC1wGXA48A1xPkBgejiSyEqGqIRGJW15VQ2Z2D/B64HbgXe6+KRz1YzN7KqrgSsGuMBGMUiIQkZjke43gBnf/ba4R7j5nEOMpOam2NMMryqiqLI87FBEpUflWDc0ws/qeHjMbbWZ/H1FMJaV5T4cuFItIrPJNBB9z9+aeHnffCXwsmpBKi5qXEJG45ZsIyizjrSlhy6Il10poFJQIRCRu+SaCB4G7zOxtZnYOcCewKLqwSkeqrVOJQERile/F4quBjwN/R9CY3EPALVEFVUp2taWZOWFU3GGISAnLKxF40DT0d8I/GUTNezp0RiAiscr3OYLpwFcJ3jRW1TPc3V8XUVwlId3Vze6OLiUCEYlVvtcIvkdwNtAJzAV+QPBwmRyGXb1PFUf5WggRkYHlmwiq3f03gLn7K+5+LcHLZOQw7GtwTjdgiUh88v0p2h42Qb3KzK4ENgBHRBdWaVCDcyKSBPmeEVwFjAA+CcwGLgYujSqoUpFSO0MikgAHPCMIHx57v7t/DmgleC+BDIJdOiMQkQQ44BmBu3cBszOfLJbBoSaoRSQJ8r1G8Axwb/h2st09A939nkiiKhGpPUoEIhK/fBPBGGAHfe8UckCJ4DA0t6UZMaycYRV5vx9IRGTQ5ftksa4LREANzolIEuT7ZPH3CM4A+nD3jw56RCVEiUBEkiDfqqH7M7qrgAuAjYMfTmlRIhCRJMi3aujuzH4zuxP4dSQRlZDUnjRTxo6IOwwRKXGHepVyOnD0YAZSinRGICJJkO81ghb6XiPYTPCOAjkMSgQikgT5Vg3VRh1Iqdnb2UVbuksvrheR2OVVNWRmF5hZXUZ/vZm9J7qwhj49VSwiSZHvNYIvunuqp8fdm4EvRhNSadilBudEJCHyTQS5ptPbVA6DzghEJCnyTQRPmdk3zewYM3udmX0LWBJlYEOdEoGIJEW+ieAfgA7gx8BdQBvwiaiCKgV6O5mIJEW+dw3tBq6JOJaS0qyWR0UkIfK9a+hhM6vP6B9tZg/m8bl5ZrbSzFab2X6JxMw+Z2ZLw79lZtZlZmMOrgjFqfftZFW61CIi8cq3amhceKcQAO6+kwO8szh8s9mNwDuAmcBFZjYzcxp3/5q7n+LupwCfB37n7q8dTAGKVaotTc3wCirK1QS1iMQr36NQt5n1NilhZlPJ0RppltOA1e7+srt3AAuB8weY/iLgzjzjKXp6qlhEksLcD3Q8D6p4gAXA78JBZwHz3b3f6iEzex8wz90vD/s/DJzu7lfmmHYEsB44NtcZgZnNB+YDNDQ0zF64cGGf8a2trdTU1BywHEnyrSXt7Gx3vvzW6j7Di7Es/RkqZRkq5QCVJakKUZa5c+cucfc5ucble7F4kZnNITgYLwXuJbhzaCC53nHcX9Z5F/D7/qqF3H0BQSJizpw53tjY2Gd8U1MT2cOS7oYVjzOxtozGxjf1GV6MZenPUCnLUCkHqCxJFXdZ8m107nLgU8AkgkTwJuAP9H11Zbb1wOSM/kn0/w6DCymhaiEIqoaOGT80fs2ISHHL9xrBp4A3Aq+4+1zgVGDbAT7zJDDdzKaZ2TCCg/192ROFbRidTXCWUTJSbWk1OCciiZDvvYvt7t5uZpjZcHd/0cyOH+gD7t5pZlcCDwLlwK3uvtzMrgjH3xROegHwUPisQsnQxWIRSYp8E8H68DmCnwMPm9lO8nhVpbs/ADyQNeymrP7bgNvyjGNIaE93sbezWw3OiUgi5Hux+IKw81ozewSoAxZFFtUQp3aGRCRJDvqxVnf/3YGnkoEoEYhIkuix1hjsa3BOiUBE4qdEEAM1OCciSaJEEANVDYlIkigRxECJQESSRIkgBqm2NGZQW6VEICLxUyKIQWpPB7XDKygvy9Uck4hIYSkRxCDVlqZOdwyJSEIoEcRAzUuISJIoEcRAiUBEkkSJIAaptjT11cPiDkNEBFAiiEWqLa0G50QkMZQICszdVTUkIomiRFBgbeku0l2uRCAiiaFEUGBqcE5EkkaJoMDU4JyIJI0SQYGpnSERSRolggJTIhCRpFEiKDAlAhFJGiWCAtvVkwh0sVhEEkKJoMCa96QpM6gZdtCvixYRiYQSQYH1PFVcpiaoRSQhlAgKTE8Vi0jSKBEUWNDgnBKBiCSHEkGBNavBORFJGCWCAtulqiERSRglggLTNQIRSRolggLqaYJaDc6JSJIoERRQ695OurrVBLWIJIsSQQGpeQkRSSIlggJSIhCRJIo0EZjZPDNbaWarzeyafqZpNLOlZrbczH4XZTxx25cI9OJ6EUmOyBq8MbNy4EbgPGA98KSZ3efuL2RMUw/8LzDP3deZ2RFRxZMEu3RGICIJFOUZwWnAand/2d07gIXA+VnTfBC4x93XAbj71gjjiV3v28l015CIJIi5ezQzNnsfwS/9y8P+DwOnu/uVGdN8G6gETgBqgevd/Qc55jUfmA/Q0NAwe+HChX3Gt7a2UlNTE0k5BtMDazq4a2Wa75w7guqK3I3OFUtZ8jFUyjJUygEqS1IVoixz585d4u5zco2Lsi3kXEe67KxTAcwG3gZUA38wsz+6+0t9PuS+AFgAMGfOHG9sbOwzk6amJrKHJdET7S9Svupl5r2tEbPciaBYypKPoVKWoVIOUFmSKu6yRJkI1gOTM/onARtzTLPd3XcDu81sMXAy8BJDUE+Dc/0lARGROER5jeBJYLqZTTOzYcCFwH1Z09wLnGlmFWY2AjgdWBFhTLFqVvMSIpJAkZ0RuHunmV0JPAiUA7e6+3IzuyIcf5O7rzCzRcBzQDdwi7sviyqmuO1Sy6MikkCRvi/R3R8AHsgadlNW/9eAr0UZRybFfkMAAAzISURBVFKk2tKMHqFnCEQkWfRkcQGpwTkRSSIlggJSE9QikkRKBAXS3e1KBCKSSEoEBdKytxN3NS8hIsmjRFAgPe0M6a4hEUkaJYIC6Wl5tF6JQEQSRomgQHobnFMiEJGEUSIokN53Eej2URFJGCWCAtHbyUQkqZQICmTfNQI9WSwiyaJEUCDNbR0MKy+jqlKrXESSRUelAulpcE5NUItI0igRFEjwVHGkbfyJiBwSJYICCRqc0/UBEUkeJYICUTtDIpJUSgQF0rxHiUBEkkmJoEB0RiAiSaVEUABd3U5Le6canBORRFIiKICWdjU4JyLJpURQAGpwTkSSTImgANTOkIgkmRJBAajlURFJMiWCAtBLaUQkyZQICkBVQyKSZEoEBZDS+4pFJMGUCAog1ZZmeEUZVZXlcYciIrIfJYICSO1JU68LxSKSUEoEBaDmJUQkyZQIIrZiU4rnNzTz2u4OvvXwS6zYlIo7JBGRPvSmlCwrNqVYtGwLG5rbmFhfzbxZDcyYUHfI81qweA17Orqoq64g1ZZmweI1zD9r2iHPU0RksOmMIEPPgXtH614m1FX1HrgP9Vf8vc9sZEPzHlraO6mqrKCuupK66koWLdsyyJGLiBw6nRGE/rytla/cv4KVm1vYsbuD0SMqOfaIGo6sq2LRss15/4J3d55et5M7/vQqP3tmPd0ODaOGc9Kk4PO1VRVsaG6LsigiIgcl0kRgZvOA64Fy4BZ3vy5rfCNwL7AmHHSPu395sOPIVd3z+iNH8cKmXSxatplFyzazamsrAEfUDmPOlNFsTrXz5NqdAIwYVs6eji7mzTqSUyePpqzM9pvnmdPHsnxjC3c+sY4XN7dQM7yCE44axbRxI5k2rqY3lpb2TibWVw92EUVEDllkicDMyoEbgfOA9cCTZnafu7+QNemj7v7OqOLoqe6pq67kyFHDWbW1hV/euZGW9jRbdnVQZnDatDF86PSZvLpzD13d+54AbuvoZPnGXWxo3sNtj6/l5kfXcETtcN44dTSptjSvG1+D4fzyuY38z29X0e1w4sQ6vvreE3n3yUex7rXdLFi8hlRbmtqqClraO0m1pfnAGydFVVwRkYMW5RnBacBqd38ZwMwWAucD2YkgUouWbaGuupLXdu/lJ09tZXdHFwYcPbaa6957IufNbGBszXBgX9KAoAqno8s5YlQV//zOGUwcPYJHXtzKg8s38+DyLXR2O394eQdd3VBRZhw7vobZU0bz1b8+qXfZMybUMf+saX3OHD7wxkm6UCwiiWLuHs2Mzd4HzHP3y8P+DwOnu/uVGdM0AncTnDFsBD7r7stzzGs+MB+goaFh9sKFC/uMb21tpaamJvtjANzy/F7GDIeWNLz4WhcTRpbRMAJa0sblJw7fb/p1u7pYsqWLHe3O2CpjdkM5R4/q+0Tw/z3XTmeXs63NGTXMmFxbRrnBa3vJOc+DMVBZis1QKctQKQeoLElViLLMnTt3ibvPyTUuyjMCyzEsO+s8DUxx91Yz+0vg58D0/T7kvgBYADBnzhxvbGzsM76pqYnsYT2eSb9Eqi3NtOpKen6rp9rSHFddSWPjcTk/c0l/JcqaZ+ZDYj3L6G+e+RqoLMVmqJRlqJQDVJakirssUd4+uh6YnNE/ieBXfy933+XurWH3A0ClmY0bzCDmzWog1ZYm1Zam2723e96shkTNU0QkLlEmgieB6WY2zcyGARcC92VOYGZHmpmF3aeF8ewYzCB66unrqivZlGqnrrrysB/oimKeIiJxiaxqyN07zexK4EGC20dvdfflZnZFOP4m4H3A35lZJ9AGXOgRXLSYMaFu0A/SUcxTRCQOkT5HEFb3PJA17KaM7huAG6KMQUREBqYmJkRESpwSgYhIiVMiEBEpcUoEIiIlLrIni6NiZtuAV7IGjwO2xxBOFFSW5Bkq5QCVJakKUZYp7j4+14iiSwS5mNlT/T06XWxUluQZKuUAlSWp4i6LqoZEREqcEoGISIkbKolgQdwBDCKVJXmGSjlAZUmqWMsyJK4RiIjIoRsqZwQiInKIlAhEREpc0ScCM5tnZivNbLWZXRN3PIfDzNaa2fNmttTMnoo7nnyZ2a1mttXMlmUMG2NmD5vZqvD/6DhjzFc/ZbnWzDaE22Vp+BKlxDOzyWb2iJmtMLPlZvapcHhRbZsBylF028XMqszsCTN7NizLl8LhsW6Tor5GYGblwEvAeQQvwnkSuMjdC/pe5MFiZmuBOe5eVA/JmNlZQCvwA3efFQ77L+A1d78uTNCj3f3qOOPMRz9luRZodfevxxnbwTKzCcAEd3/azGqBJcB7gMsoom0zQDneT5Ftl/D9KyPDtzJWAo8BnwLeS4zbpNjPCE4DVrv7y+7eASwEzo85ppLj7ouB17IGnw98P+z+PsEXN/H6KUtRcvdN7v502N0CrAAmUmTbZoByFB0PtIa9leGfE/M2KfZEMBF4NaN/PUW6g4QceMjMlpjZ/LiDOUwN7r4Jgi8ycETM8RyuK83subDqKNFVKbmY2VTgVOBPFPG2ySoHFOF2MbNyM1sKbAUedvfYt0mxJwLLMax467rgre7+BuAdwCfCagqJ33eAY4BTgE3AN+IN5+CYWQ1wN3CVu++KO55DlaMcRbld3L3L3U8heI/7aWY2K+6Yij0RrAcmZ/RPAjbGFMthc/eN4f+twM8Iqr6K1ZawbrenjndrzPEcMnffEn55u4GbKaLtEtZD3w38yN3vCQcX3bbJVY5i3i4A7t4MNAHziHmbFHsieBKYbmbTzGwYcCFwX8wxHRIzGxleCMPMRgJvB5YN/KlEuw+4NOy+FLg3xlgOS88XNHQBRbJdwguT3wVWuPs3M0YV1bbprxzFuF3MbLyZ1Yfd1cC5wIvEvE2K+q4hgPCWsW8D5cCt7v7vMYd0SMzsdQRnARC8S/qOYimLmd0JNBI0pbsF+CLwc+Au4GhgHfA37p74i7D9lKWRoPrBgbXAx3vqc5PMzM4AHgWeB7rDwV8gqF8vmm0zQDkuosi2i5mdRHAxuJzgh/hd7v5lMxtLjNuk6BOBiIgcnmKvGhIRkcOkRCAiUuKUCERESpwSgYhIiVMiEBEpcUoEUtLM7CozG5HR/0DPfd5xzGeAeV8SdjeZ2X4vOTezd/e0vmtm7zGzmQeY5zt7Wr4U0e2jUtIGq8XXqFqONbMK4GngDe7eaWZNwGfdvd9mys3sNuB+d//pANNYON+3uvuewYxZio/OCCSxzOyfLHjXxK/N7E4z+2w4vPdXsZmNCw/CmNlUM3vUzJ4O/94SDm8MP/NTM3vRzH5kgU8CRwGPmNkj4bRrw3leYfvauV+TMf47ZvaU9W1Lvt/5hN3/z8yWhX9XZcS6wsxuDuf1UPikabZzgKfdvTNj2MVm9ng4v9PC+V1mZjeEZX438LUw9mPM7JNm9kLYONtCCFrBJGje4J2DsrGkuLm7/vSXuD9gNsGTpCOAUcBqgl/CEBzA5oTd44C1YfcIoCrsng48FXY3AimCtqjKgD8AZ4Tj1gLjMpab3V9J8FTru8L+MeH/8jCOkwaaT0Y5RgI1wHKC1jOnAp3AKeH0dwEX51gPXwL+IaO/Cbg57D4LWBZ2XwbcEHbfBrwv4zMbgeFhd33G8A8B/xP3ttZf/H86I5CkOhP4mbvv8aClyXzakKoEbjaz54GfAJn15E+4+3oPGihbSnAgzsf1wG/d/Rdh//vN7GngGeCErGXkckZYjt0etEN/T1g2gDXuvjTsXtJPTBOAbVnD7oTedyeMyuNaxHPAj8zsYoLk02MrwZmMlDglAkmy/i5gdbJv363KGP5pgvaBTgbmAMMyxu3N6O4iaM9pQGZ2GTCF4Fc5ZjYN+CzwNnc/Cfhl1vJzzmaAcfnE1JZjGdnr5UAX+v4KuJHg7GRJeN2BcL5tB/islAAlAkmqxcAFZlYdtsr6roxxawkOagDvyxheB2wKf/V/mKD65kBagNrsgWY2m+Cgf3E4PwiqqHYDKTNrIHhvxIDzCcvxHjMbEbYqewFBVVO+VgDHZg37QBjjGUDK3VP9lcnMyoDJ7v4I8I9APUEVFcBxFEGLnRI9JQJJJA9eTfhjgmqcu+l78Pw68Hdm9jhBPXyP/wUuNbM/EhzkduexqAXAr3ou8ma4EhhDcAF4qZnd4u7PElQJLQduBX5/oPmE5bgNeIKg1c9b3P2ZPOLq8SuCawGZdoZlvwn42xyfWQh8zsyeIbhW8sOwuuwZ4FsetIMPMJfgrEZKnG4flaJgRfoC+cFgZj8D/tHdVw3iPBsImjp/22DNU4qXzghEku8agovGg+lo4DODPE8pUjojEBEpcTojEBEpcUoEIiIlTolARKTEKRGIiJQ4JQIRkRL3/wG3ivCD4kbhvwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data for plotting\n",
    "t = np.array(bits_list)\n",
    "s = np.array(test_accuracies_list)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t, s)\n",
    "plt.plot(t, s, 'C0o', alpha=0.5)\n",
    "\n",
    "ax.set(xlabel='quantization (bits)', ylabel='accuracy',\n",
    "       title='Accuracy curve for different quantization levels')\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(\"acc_over_bits.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate model and save parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing for 6 bits\n",
      "Evaluating model\n",
      "----------\n",
      "Test batch 240/250\n",
      "Evaluation completed in 1m 42s\n",
      "Avg loss (test): 0.0114\n",
      "Avg acc (test): 0.9695\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "bits_quantization = 6\n",
    "\n",
    "print(\"Quantizing for {} bits\".format(bits_quantization))\n",
    "# Quantize weights\n",
    "squeezenet1_1 = create_model()\n",
    "state_dict = squeezenet1_1.state_dict()\n",
    "state_dict_quant = quantize_state_dict(state_dict, bits_quantization, bits_quantization)\n",
    "#print(state_dict_quant)\n",
    "squeezenet1_1_temp = create_model()\n",
    "squeezenet1_1_temp.load_state_dict(state_dict_quant)\n",
    "\n",
    "# Quantize forward activation\n",
    "squeezenet1_1_quant = quantize_model_forward_activation(squeezenet1_1_temp, bits_quantization)\n",
    "#print()\n",
    "#print(squeezenet1_1_quant)\n",
    "\n",
    "# evaluate\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "avg_acc, avg_loss = eval_model(squeezenet1_1_quant, criterion, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(squeezenet1_1_quant.state_dict(), 'weights/squeezenet_v1-flower-or-crops_quantized-floats.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_json(dictionary, json_filename):\n",
    "    import json\n",
    "\n",
    "    with open(json_filename, 'w') as fp:\n",
    "        json.dump(dictionary, fp, indent=4)\n",
    "        \n",
    "# Print model's state_dict\n",
    "def untensorize_dict(tensor_dict):\n",
    "    \"\"\"Converts a dictionary generated directly by pytorch's model.state_dict() method into \n",
    "    a dictionary with lists instead of tensors\"\"\"\n",
    "    untensored_dict = {}\n",
    "    for param_tensor in tensor_dict:\n",
    "        untensored_numpy = tensor_dict[param_tensor].cpu().numpy()\n",
    "        untensored_dict[param_tensor] = untensored_numpy.tolist() # converts numpy array to nested lists\n",
    "    return untensored_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_quant_floats_untensored = untensorize_dict(state_dict_quant)\n",
    "dict_to_json(state_dict_quant_floats_untensored, 'weights/squeezenet_v1-flower-or-crops_quantized-floats.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the quantized state dict with integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_quant_ints = quantize_state_dict(state_dict, bits_quantization, bits_quantization, return_type='int')\n",
    "\n",
    "state_dict_quant_ints_untensorized = untensorize_dict(state_dict_quant_ints)\n",
    "dict_to_json(state_dict_quant_ints_untensorized, 'weights/squeezenet_v1-flower-or-crops_quantized-ints.json')"
   ]
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the input and output of an intermediate layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the discussion available at https://forums.fast.ai/t/pytorch-best-way-to-get-at-intermediate-layers-in-vgg-and-resnet/5707"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (0_Conv2d_quant): LinearQuant(sf=2, bits=6, overflow_rate=0.000, counter=0)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (3): Fire_Quant(\n",
      "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_quant): LinearQuant(sf=1, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_quant): LinearQuant(sf=1, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_quant): LinearQuant(sf=1, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (4): Fire_Quant(\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_quant): LinearQuant(sf=0, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_quant): LinearQuant(sf=0, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_quant): LinearQuant(sf=-1, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (6): Fire_Quant(\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_quant): LinearQuant(sf=-1, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_quant): LinearQuant(sf=-1, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_quant): LinearQuant(sf=-2, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (7): Fire_Quant(\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_quant): LinearQuant(sf=-2, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_quant): LinearQuant(sf=-1, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_quant): LinearQuant(sf=-2, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (8): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (9): Fire_Quant(\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_quant): LinearQuant(sf=-3, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_quant): LinearQuant(sf=-2, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_quant): LinearQuant(sf=-4, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (10): Fire_Quant(\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_quant): LinearQuant(sf=-3, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_quant): LinearQuant(sf=-3, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_quant): LinearQuant(sf=-4, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (11): Fire_Quant(\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_quant): LinearQuant(sf=-4, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_quant): LinearQuant(sf=-3, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_quant): LinearQuant(sf=-3, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (12): Fire_Quant(\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_quant): LinearQuant(sf=-3, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_quant): LinearQuant(sf=-3, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_quant): LinearQuant(sf=-3, bits=6, overflow_rate=0.000, counter=0)\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1_Conv2d_quant): LinearQuant(sf=-2, bits=6, overflow_rate=0.000, counter=0)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# check current squeezenet structure to decide where to sample the input and output data\n",
    "print(squeezenet1_1_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conv2d data will be acquired from the first squeeze layer of the first Fire layer module. The input is the input being inserted on the fire module, and the output the data coming out of the squeeze_quant layer (after the quantization therefore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set hook for input capture of \"Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\"\n",
      "Set hook for output capture of \"LinearQuant(sf=1, bits=6, overflow_rate=0.000, counter=0)\"\n",
      "----\n",
      "Label is \"0\"\n",
      "Prediction is \"0\"\n"
     ]
    }
   ],
   "source": [
    "layer_outputs = [0]\n",
    "layer_inputs = [0]\n",
    "\n",
    "def hook_conv2d_output(module, input, output):\n",
    "    layer_outputs[0] = output\n",
    "\n",
    "def hook_conv2d_input(module, input, output):\n",
    "    layer_inputs[0] = input\n",
    "\n",
    "data, label = dataloaders[TEST].dataset[0]\n",
    "\n",
    "if data.dim() == 3:\n",
    "    # single input sample, force to a 4-dim input\n",
    "    data = data.unsqueeze(0)\n",
    "\n",
    "# Set hook to get the input to the conv2d layer\n",
    "print('Set hook for input capture of \"{}\"'.format(squeezenet1_1_quant.features[4].squeeze))\n",
    "squeezenet1_1_quant.features[4].squeeze.register_forward_hook(hook_conv2d_input)\n",
    "\n",
    "# Set hook to get the output of the quantized layer (after conv2d)\n",
    "print('Set hook for output capture of \"{}\"'.format(squeezenet1_1_quant.features[4].squeeze_quant))\n",
    "squeezenet1_1_quant.features[4].squeeze_quant.register_forward_hook(hook_conv2d_output)\n",
    "\n",
    "\n",
    "out = squeezenet1_1_quant(data)\n",
    "\n",
    "# Check if prediction matches the label\n",
    "_, preds = torch.max(out.data, 1)\n",
    "print('----')\n",
    "print('Label is \"{}\"'.format(label))\n",
    "print('Prediction is \"{}\"'.format(preds.item()))"
   ]
  },
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
