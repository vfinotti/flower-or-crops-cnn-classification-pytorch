{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16000 images under train\n",
      "Loaded 2000 images under test\n",
      "Classes: \n",
      "['flower', 'sugarcane']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "## DATA LOADER\n",
    "data_dir = './images'\n",
    "TRAIN = 'train'\n",
    "VAL = 'val'\n",
    "TEST = 'test'\n",
    "\n",
    "# Squeezenet Takes 224x224 images as input, so we resize all of them\n",
    "data_transforms = {\n",
    "    TRAIN: transforms.Compose([\n",
    "        # Data augmentation is a good practice for the train set\n",
    "        # Here, we randomly crop the image to 224x224 and\n",
    "        # randomly flip it horizontally.\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    # VAL: transforms.Compose([\n",
    "    #     transforms.Resize(256),\n",
    "    #     transforms.CenterCrop(224),\n",
    "    #     transforms.ToTensor(),\n",
    "    # ]),\n",
    "    TEST: transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "    x: datasets.ImageFolder(\n",
    "        os.path.join(data_dir, x),\n",
    "        transform=data_transforms[x]\n",
    "    )\n",
    "    for x in [TRAIN, TEST]\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(\n",
    "        image_datasets[x], batch_size=8,\n",
    "        shuffle=True, num_workers=4\n",
    "    )\n",
    "    for x in [TRAIN, TEST]\n",
    "}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [TRAIN, TEST]}\n",
    "\n",
    "for x in [TRAIN, TEST]:\n",
    "    print(\"Loaded {} images under {}\".format(dataset_sizes[x], x))\n",
    "\n",
    "print(\"Classes: \")\n",
    "class_names = image_datasets[TRAIN].classes\n",
    "print(image_datasets[TRAIN].classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization classes and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is based on the code available at https://github.com/aaron-xichen/pytorch-playground. Although the refered repository offers some options for quantizing popular CNN architectures like Squeezenet, VGG, Alexnet and Resnet, a more dedicated code for my application was necessary in my opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "\n",
    "def compute_integral_part(input, overflow_rate):\n",
    "    \"\"\"Calculates the scaling factor (sf) that better represents the input\"\"\"\n",
    "    # transform 'input' into an array of the abs of each elements \n",
    "    abs_value = input.abs().view(-1)\n",
    "    # sort the modulus array in descending order\n",
    "    sorted_value = abs_value.sort(dim=0, descending=True)[0]\n",
    "    # find what index corresponds to the max possibe modulus value, considering the overflow_rate.\n",
    "    # for '0' overflow_rate, the index will be the one of the maximum module of all modules, and \n",
    "    # the biggest modulus (index 0) will be chosen\n",
    "    split_idx = int(overflow_rate * len(sorted_value))\n",
    "    # value at that index\n",
    "    v = sorted_value[split_idx]\n",
    "    #print('v is {}'.format(v))\n",
    "    if isinstance(v, Variable):\n",
    "        v = v.data.cpu().numpy()\n",
    "    # get the minimum ammount of bits required to represent the value chosen and consider it the \n",
    "    # scaling factor. The '1e-12' is there to determine the smallest precision (if 'v' is too small)\n",
    "    sf = math.ceil(math.log2(v+1e-12))\n",
    "    #print('sf is {}'.format(sf))\n",
    "    return sf\n",
    "\n",
    "def linear_quantize(input, sf, bits):\n",
    "    \"\"\"Converts a float value from the real numbers domain to a float in the quantized domain\"\"\"\n",
    "    assert bits >= 1, bits\n",
    "    if bits == 1:\n",
    "        return torch.sign(input) - 1\n",
    "    #print('inside sf is {}'.format(sf))\n",
    "    # calculate the minimum step, considering that the 'sf' bits will quantize in the interval [0,1].\n",
    "    # this is equivalento to compute 1/(2^(sf)),  or 2^(-sf)\n",
    "    delta = math.pow(2.0, -sf)\n",
    "    bound = math.pow(2.0, bits-1)\n",
    "    # calculates min and maximum. For 8 bits, the quantized number will be between [-128,127]. \n",
    "    min_val = - bound\n",
    "    max_val = bound - 1\n",
    "    \n",
    "    # dividing the input by delta and flooring\n",
    "    ## rounded = torch.floor(input / delta + 0.5) # Equivalent to torch.round(input / delta)\n",
    "    rounded = torch.round(input / delta)\n",
    "\n",
    "    clipped_value = torch.clamp(rounded, min_val, max_val) * delta\n",
    "    return clipped_value\n",
    "\n",
    "\n",
    "class LinearQuant(nn.Module):\n",
    "    def __init__(self, name, bits, sf=None, overflow_rate=0.0, counter=10):\n",
    "        super(LinearQuant, self).__init__()\n",
    "        self.name = name\n",
    "        self._counter = counter\n",
    "\n",
    "        self.bits = bits\n",
    "        self.sf = sf\n",
    "        self.overflow_rate = overflow_rate\n",
    "\n",
    "    @property\n",
    "    def counter(self):\n",
    "        return self._counter\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self._counter > 0:\n",
    "            self._counter -= 1\n",
    "            sf_new = self.bits - 1 - compute_integral_part(input, self.overflow_rate)\n",
    "            self.sf = min(self.sf, sf_new) if self.sf is not None else sf_new\n",
    "            return input\n",
    "        else:\n",
    "            output = linear_quantize(input, self.sf, self.bits)\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(sf={}, bits={}, overflow_rate={:.3f}, counter={})'.format(\n",
    "self.__class__.__name__, self.sf, self.bits, self.overflow_rate, self.counter)\n",
    "\n",
    "\n",
    "def duplicate_model_with_quant(model, bits, overflow_rate=0.0, counter=10):\n",
    "    \"\"\"assume that original model has at least a nn.Sequential\"\"\"\n",
    "    if isinstance(model, nn.Sequential):\n",
    "        l = OrderedDict()\n",
    "        for k, v in model._modules.items():\n",
    "            if isinstance(v, (nn.Conv2d, nn.Linear, nn.BatchNorm1d, nn.BatchNorm2d, nn.AvgPool2d)):\n",
    "                l[k] = v\n",
    "#                if type == 'linear':\n",
    "#                    quant_layer = LinearQuant('{}_quant'.format(k), bits=bits, overflow_rate=overflow_rate, counter=counter)\n",
    "#                elif type == 'log':\n",
    "#                    # quant_layer = LogQuant('{}_quant'.format(k), bits=bits, overflow_rate=overflow_rate, counter=counter)\n",
    "#                    quant_layer = NormalQuant('{}_quant'.format(k), bits=bits, quant_func=log_minmax_quantize)\n",
    "#                elif type == 'minmax':\n",
    "#                    quant_layer = NormalQuant('{}_quant'.format(k), bits=bits, quant_func=min_max_quantize)\n",
    "#                else:\n",
    "#                    quant_layer = NormalQuant('{}_quant'.format(k), bits=bits, quant_func=tanh_quantize)\n",
    "                quant_layer = LinearQuant('{}_quant'.format(k), bits=bits, overflow_rate=overflow_rate, counter=counter)\n",
    "                l['{}_{}_quant'.format(k, type)] = quant_layer\n",
    "            else:\n",
    "                l[k] = duplicate_model_with_quant(v, bits, overflow_rate, counter)\n",
    "        m = nn.Sequential(l)\n",
    "        return m\n",
    "    else:\n",
    "        for k, v in model._modules.items():\n",
    "            model._modules[k] = duplicate_model_with_quant(v, bits, overflow_rate, counter)\n",
    "    return model\n",
    "\n",
    "def eval_model(squeezenet, criterion, verbose=False):\n",
    "    since = time.time()\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    loss_test = 0\n",
    "    acc_test = 0\n",
    "\n",
    "    test_batches = len(dataloaders[TEST])\n",
    "    if verbose:\n",
    "        print(\"Evaluating model\")\n",
    "        print('-' * 10)\n",
    "\n",
    "    squeezenet.train(False)\n",
    "    squeezenet.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders[TEST]):\n",
    "            if verbose:\n",
    "                if i % 10 == 0:\n",
    "                    print(\"\\rTest batch {}/{}\".format(i, test_batches), end='', flush=True)\n",
    "\n",
    "\n",
    "            inputs, labels = data\n",
    "            if use_gpu:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            else:\n",
    "                inputs, labels = inputs, labels\n",
    "\n",
    "            outputs = squeezenet(inputs)\n",
    "\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss_test += loss.data\n",
    "            acc_test += torch.sum(preds == labels.data).item()\n",
    "\n",
    "            # del inputs, labels, outputs, preds\n",
    "            # torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = loss_test / dataset_sizes[TEST]\n",
    "    avg_acc = acc_test / dataset_sizes[TEST]\n",
    "\n",
    "    elapsed_time = time.time() - since\n",
    "    if verbose:\n",
    "        print()\n",
    "        print(\"Evaluation completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n",
    "        print(\"Avg loss (test): {:.4f}\".format(avg_loss))\n",
    "        print(\"Avg acc (test): {:.4f}\".format(avg_acc))\n",
    "        print('-' * 10)\n",
    "    return avg_acc, avg_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.6000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "0\n",
      "v is 5\n",
      "2.321928094887651\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, 3)\n",
    "x[0,2] = 1\n",
    "x[0,2] = 3.6\n",
    "abs_values = x.abs().view(-1)\n",
    "sorted_values = abs_values.sort(dim=0, descending=True)[0]\n",
    "print(sorted_values)\n",
    "\n",
    "overflow_rate = 0.0\n",
    "split_idx = int(overflow_rate * len(sorted_values))\n",
    "print(split_idx)\n",
    "\n",
    "v = sorted_values[split_idx]\n",
    "v = 5\n",
    "print(\"v is {}\".format(v))\n",
    "a = math.log2(v+1e-12)\n",
    "print(a)\n",
    "\n",
    "sf = compute_integral_part(x, 0.0)\n",
    "print(sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sf is 5\n",
      "post-calculated sf is 2.0\n",
      "delta is 0.25\n",
      "a ([32.]) quantized is [128.]\n",
      "a ([32.]) rounded is [32.]\n"
     ]
    }
   ],
   "source": [
    "bits = 8\n",
    "\n",
    "x = torch.ones(1, 1)\n",
    "x[0,0] = 25\n",
    "\n",
    "sf = compute_integral_part(x, 0.0)\n",
    "print('sf is {}'.format(sf))\n",
    "\n",
    "sf = bits - 1. - sf\n",
    "print('post-calculated sf is {}'.format(sf))\n",
    "\n",
    "delta = math.pow(2.0, -sf)\n",
    "print('delta is {}'.format(delta))\n",
    "\n",
    "a = torch.Tensor(1)\n",
    "a[0] = 32\n",
    "quantized = torch.round(a / delta)\n",
    "print('a ({}) quantized is {}'.format(a.numpy(), quantized.numpy()))\n",
    "rounded = quantized*delta\n",
    "print('a ({}) rounded is {}'.format(a.numpy(), rounded.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark =True\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "#parser = argparse.ArgumentParser(description='PyTorch SVHN Example')\n",
    "# parser.add_argument('--type', default='cifar10', help='|'.join(selector.known_models))\n",
    "# parser.add_argument('--quant_method', default='linear', help='linear|minmax|log|tanh')\n",
    "#parser.add_argument('--batch_size', type=int, default=100, help='input batch size for training (default: 64)')\n",
    "#parser.add_argument('--gpu', default=None, help='index of gpus to use')\n",
    "#parser.add_argument('--ngpu', type=int, default=8, help='number of gpus to use')\n",
    "#parser.add_argument('--seed', type=int, default=117, help='random seed (default: 1)')\n",
    "# parser.add_argument('--model_root', default='~/.torch/models/', help='folder to save the model')\n",
    "# parser.add_argument('--data_root', default='/tmp/public_dataset/pytorch/', help='folder to save the model')\n",
    "# parser.add_argument('--logdir', default='log/default', help='folder to save to the log')\n",
    "\n",
    "#parser.add_argument('--input_size', type=int, default=224, help='input size of image')\n",
    "# parser.add_argument('--n_sample', type=int, default=20, help='number of samples to infer the scaling factor')\n",
    "#parser.add_argument('--param_bits', type=int, default=8, help='bit-width for parameters')\n",
    "#parser.add_argument('--bn_bits', type=int, default=32, help='bit-width for running mean and std')\n",
    "#parser.add_argument('--fwd_bits', type=int, default=8, help='bit-width for layer output')\n",
    "# parser.add_argument('--overflow_rate', type=float, default=0.0, help='overflow rate')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "args = {}\n",
    "\n",
    "args['batch_size'] = 100\n",
    "args['gpu'] = None\n",
    "args['ngpu'] = 8\n",
    "args['seed'] = 117\n",
    "args['input_size'] = 224\n",
    "#args['param_bits'] = 2\n",
    "#args['bn_bits'] = 2\n",
    "#args['fwd_bits'] = 2\n",
    "args['overflow_rate'] = 0.0\n",
    "\n",
    "#args.gpu = misc.auto_select_gpu(utility_bound=0, num_gpu=args.ngpu, selected_gpus=args.gpu)\n",
    "#args.ngpu = len(args.gpu)\n",
    "#misc.ensure_dir(args.logdir)\n",
    "#args.model_root = misc.expand_user(args.model_root)\n",
    "#args.data_root = misc.expand_user(args.data_root)\n",
    "#args.input_size = 299 if 'inception' in args.type else args.input_size\n",
    "#assert args.quant_method in ['linear', 'minmax', 'log', 'tanh']\n",
    "#print(\"=================FLAGS==================\")\n",
    "#for k, v in args.__dict__.items():\n",
    "#    print('{}: {}'.format(k, v))\n",
    "#print(\"========================================\")\n",
    "\n",
    "#assert torch.cuda.is_available(), 'no cuda'\n",
    "torch.manual_seed(args['seed'])\n",
    "torch.cuda.manual_seed(args['seed'])\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu:\n",
    "    print(\"Using CUDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model():\n",
    "    # load model and dataset fetcher\n",
    "    # model_raw, ds_fetcher, is_imagenet = selector.select(args.type, model_root=args.model_root)\n",
    "    #squeezenet1_1 = models.squeezenet1_1()\n",
    "\n",
    "    # importing fixed version of squeezenet class and functions\n",
    "    import squeezenet_fix\n",
    "\n",
    "    squeezenet1_1 = squeezenet_fix.squeezenet1_1()\n",
    "\n",
    "    # Freeze training for all layers\n",
    "    for param in squeezenet1_1.features.parameters():\n",
    "        param.require_grad = False\n",
    "\n",
    "    # Newly created modules have require_grad=True by default\n",
    "    num_features = squeezenet1_1.classifier[1].in_channels\n",
    "    features = list(squeezenet1_1.classifier.children())[:-3] # Remove last 3 layers\n",
    "    features.extend([nn.Conv2d(num_features, 2, kernel_size=1)]) # Add\n",
    "    features.extend([nn.ReLU(inplace=True)]) # Add\n",
    "    features.extend([nn.AdaptiveAvgPool2d(output_size=(1,1))]) # Add our layer with 2 outputs\n",
    "    squeezenet1_1.classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "\n",
    "    if use_gpu:\n",
    "        squeezenet1_1.load_state_dict(torch.load('./weights/squeezenet_v1-flower-or-crops.pt'))\n",
    "    else:\n",
    "        squeezenet1_1.load_state_dict(torch.load('./weights/squeezenet_v1-flower-or-crops.pt', map_location='cpu'))\n",
    "    #print(squeezenet1_1)\n",
    "    return squeezenet1_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_state_dict(state_dict, bn_bits, param_bits):\n",
    "    if param_bits < 32:\n",
    "        state_dict_quant = OrderedDict()\n",
    "        sf_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            if 'running' in k:\n",
    "                if bn_bits >=32:\n",
    "                    print(\"Ignoring {}\".format(k))\n",
    "                    state_dict_quant[k] = v\n",
    "                    continue\n",
    "                else:\n",
    "                    bits = bn_bits\n",
    "            else:\n",
    "                bits = param_bits\n",
    "\n",
    "    #        if args.quant_method == 'linear':\n",
    "    #            sf = bits - 1. - quant.compute_integral_part(v, overflow_rate=args.overflow_rate)\n",
    "    #            v_quant  = quant.linear_quantize(v, sf, bits=bits)\n",
    "    #        elif args.quant_method == 'log':\n",
    "    #            v_quant = quant.log_minmax_quantize(v, bits=bits)\n",
    "    #        elif args.quant_method == 'minmax':\n",
    "    #            v_quant = quant.min_max_quantize(v, bits=bits)\n",
    "    #        else:\n",
    "    #            v_quant = quant.tanh_quantize(v, bits=bits)\n",
    "    \n",
    "            # The sf will be used to do the quantization. Subtract 1 for dividind the range by 2\n",
    "            # (2^(-sf) will be calculated after), so half of the quatized range represents positive\n",
    "            # numbers and the other half negative numbers. Subtract the ammount of bits required to\n",
    "            # represent the max abs value of the input to adjust the scale. At the end of the day, \n",
    "            # the operation done through these steps is equivalent to consider that you have \"bits - 1\"\n",
    "            # bits to quantize the maximum modulus of the input array. I don't know why to make such simple\n",
    "            # operation not explicit...\n",
    "            sf = bits - 1. - compute_integral_part(v, overflow_rate=args['overflow_rate'])\n",
    "            #sf = compute_integral_part(v, overflow_rate=args['overflow_rate'])\n",
    "            v_quant  = linear_quantize(v, sf, bits=bits)     \n",
    "            state_dict_quant[k] = v_quant\n",
    "        return state_dict_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "\n",
    "\n",
    "def quantize_model_forward_activation(model, fwd_bits):\n",
    "    # Quantize the forward activaton of parameters on the model\n",
    "    if fwd_bits < 32:\n",
    "        model = duplicate_model_with_quant(model, bits=fwd_bits)\n",
    "        #print(squeezenet1_1)\n",
    "        #val_ds_tmp = ds_fetcher(10, data_root=args.data_root, train=False, input_size=args.input_size)\n",
    "        #misc.eval_model(model_raw, val_ds_tmp, ngpu=1, n_sample=args.n_sample, is_imagenet=is_imagenet)\n",
    "        if use_gpu:\n",
    "            model.cuda() #.cuda() will move everything to the GPU side\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse quantization vs accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse the accuracy on the test set over different levels of quantization. With this information, it is possible to choose the most suitable quantization level for the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Tests 31/31"
     ]
    }
   ],
   "source": [
    "bits_list = list(range(1, 32))\n",
    "#bits_list = list(range(8, 9))\n",
    "\n",
    "test_accuracies_list = []\n",
    "test_losses_list = []\n",
    "\n",
    "for idx, bits_item in enumerate(bits_list):\n",
    "    print(\"\\rRunning Tests {}/{}\".format(idx+1, len(bits_list)), end='', flush=True)\n",
    "    # Quantize weights\n",
    "    squeezenet1_1 = create_model()\n",
    "    state_dict = squeezenet1_1.state_dict()\n",
    "    state_dict_quant = quantize_state_dict(state_dict, bits_item, bits_item)\n",
    "    #print(state_dict_quant)\n",
    "    squeezenet1_1.load_state_dict(state_dict_quant)\n",
    "\n",
    "    # Quantize forward activation\n",
    "    squeezenet1_1_quant = quantize_model_forward_activation(squeezenet1_1, bits_item)\n",
    "    #print()\n",
    "    #print(squeezenet1_1_quant)\n",
    "\n",
    "    # evaluate\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    #exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "    avg_acc, avg_loss = eval_model(squeezenet1_1_quant, criterion)\n",
    "    test_accuracies_list.append(avg_acc)\n",
    "    test_losses_list.append(avg_loss)\n",
    "    \n",
    "    #print()\n",
    "    #print(idx)\n",
    "    #print(bits_item)\n",
    "    #print(avg_acc)\n",
    "    #print(test_accuracies_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data for plotting\n",
    "t = np.array(bits_list)\n",
    "s = np.array(test_accuracies_list)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t, s)\n",
    "plt.plot(t, s, 'C0o', alpha=0.5)\n",
    "\n",
    "ax.set(xlabel='quantization (bits)', ylabel='accuracy',\n",
    "       title='Accuracy curve for different quantization levels')\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(\"acc_over_bits.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
