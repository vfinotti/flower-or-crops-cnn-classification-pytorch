{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16000 images under train\n",
      "Loaded 2000 images under test\n",
      "Classes: \n",
      "['flower', 'sugarcane']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "## DATA LOADER\n",
    "data_dir = './images'\n",
    "TRAIN = 'train'\n",
    "VAL = 'val'\n",
    "TEST = 'test'\n",
    "\n",
    "# Squeezenet Takes 224x224 images as input, so we resize all of them\n",
    "data_transforms = {\n",
    "    TRAIN: transforms.Compose([\n",
    "        # Data augmentation is a good practice for the train set\n",
    "        # Here, we randomly crop the image to 224x224 and\n",
    "        # randomly flip it horizontally.\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    # VAL: transforms.Compose([\n",
    "    #     transforms.Resize(256),\n",
    "    #     transforms.CenterCrop(224),\n",
    "    #     transforms.ToTensor(),\n",
    "    # ]),\n",
    "    TEST: transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "    x: datasets.ImageFolder(\n",
    "        os.path.join(data_dir, x),\n",
    "        transform=data_transforms[x]\n",
    "    )\n",
    "    for x in [TRAIN, TEST]\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(\n",
    "        image_datasets[x], batch_size=8,\n",
    "        shuffle=True, num_workers=4\n",
    "    )\n",
    "    for x in [TRAIN, TEST]\n",
    "}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [TRAIN, TEST]}\n",
    "\n",
    "for x in [TRAIN, TEST]:\n",
    "    print(\"Loaded {} images under {}\".format(dataset_sizes[x], x))\n",
    "\n",
    "print(\"Classes: \")\n",
    "class_names = image_datasets[TRAIN].classes\n",
    "print(image_datasets[TRAIN].classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is based on the code available at https://github.com/aaron-xichen/pytorch-playground. Although the refered repository offers some options for quantizing popular CNN architectures like Squeezenet, VGG, Alexnet and Resnet, a more dedicated code for my application was necessary in my opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "\n",
    "def compute_integral_part(input, overflow_rate):\n",
    "    abs_value = input.abs().view(-1)\n",
    "    sorted_value = abs_value.sort(dim=0, descending=True)[0]\n",
    "    split_idx = int(overflow_rate * len(sorted_value))\n",
    "    v = sorted_value[split_idx]\n",
    "    if isinstance(v, Variable):\n",
    "        v = v.data.cpu().numpy()\n",
    "    sf = math.ceil(math.log2(v+1e-12))\n",
    "    return sf\n",
    "\n",
    "def linear_quantize(input, sf, bits):\n",
    "    assert bits >= 1, bits\n",
    "    if bits == 1:\n",
    "        return torch.sign(input) - 1\n",
    "    delta = math.pow(2.0, -sf)\n",
    "    bound = math.pow(2.0, bits-1)\n",
    "    min_val = - bound\n",
    "    max_val = bound - 1\n",
    "    rounded = torch.floor(input / delta + 0.5)\n",
    "\n",
    "    clipped_value = torch.clamp(rounded, min_val, max_val) * delta\n",
    "    return clipped_value\n",
    "\n",
    "\n",
    "class LinearQuant(nn.Module):\n",
    "    def __init__(self, name, bits, sf=None, overflow_rate=0.0, counter=10):\n",
    "        super(LinearQuant, self).__init__()\n",
    "        self.name = name\n",
    "        self._counter = counter\n",
    "\n",
    "        self.bits = bits\n",
    "        self.sf = sf\n",
    "        self.overflow_rate = overflow_rate\n",
    "\n",
    "    @property\n",
    "    def counter(self):\n",
    "        return self._counter\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self._counter > 0:\n",
    "            self._counter -= 1\n",
    "            sf_new = self.bits - 1 - compute_integral_part(input, self.overflow_rate)\n",
    "            self.sf = min(self.sf, sf_new) if self.sf is not None else sf_new\n",
    "            return input\n",
    "        else:\n",
    "            output = linear_quantize(input, self.sf, self.bits)\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(sf={}, bits={}, overflow_rate={:.3f}, counter={})'.format(\n",
    "self.__class__.__name__, self.sf, self.bits, self.overflow_rate, self.counter)\n",
    "\n",
    "\n",
    "def duplicate_model_with_quant(model, bits, overflow_rate=0.0, counter=10):\n",
    "    \"\"\"assume that original model has at least a nn.Sequential\"\"\"\n",
    "    if isinstance(model, nn.Sequential):\n",
    "        l = OrderedDict()\n",
    "        for k, v in model._modules.items():\n",
    "            if isinstance(v, (nn.Conv2d, nn.Linear, nn.BatchNorm1d, nn.BatchNorm2d, nn.AvgPool2d)):\n",
    "                l[k] = v\n",
    "#                if type == 'linear':\n",
    "#                    quant_layer = LinearQuant('{}_quant'.format(k), bits=bits, overflow_rate=overflow_rate, counter=counter)\n",
    "#                elif type == 'log':\n",
    "#                    # quant_layer = LogQuant('{}_quant'.format(k), bits=bits, overflow_rate=overflow_rate, counter=counter)\n",
    "#                    quant_layer = NormalQuant('{}_quant'.format(k), bits=bits, quant_func=log_minmax_quantize)\n",
    "#                elif type == 'minmax':\n",
    "#                    quant_layer = NormalQuant('{}_quant'.format(k), bits=bits, quant_func=min_max_quantize)\n",
    "#                else:\n",
    "#                    quant_layer = NormalQuant('{}_quant'.format(k), bits=bits, quant_func=tanh_quantize)\n",
    "                quant_layer = LinearQuant('{}_quant'.format(k), bits=bits, overflow_rate=overflow_rate, counter=counter)\n",
    "                l['{}_{}_quant'.format(k, type)] = quant_layer\n",
    "            else:\n",
    "                l[k] = duplicate_model_with_quant(v, bits, overflow_rate, counter)\n",
    "        m = nn.Sequential(l)\n",
    "        return m\n",
    "    else:\n",
    "        for k, v in model._modules.items():\n",
    "            model._modules[k] = duplicate_model_with_quant(v, bits, overflow_rate, counter)\n",
    "    return model\n",
    "\n",
    "def eval_model(squeezenet, criterion, verbose=False):\n",
    "    since = time.time()\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    loss_test = 0\n",
    "    acc_test = 0\n",
    "\n",
    "    test_batches = len(dataloaders[TEST])\n",
    "    if verbose:\n",
    "        print(\"Evaluating model\")\n",
    "        print('-' * 10)\n",
    "\n",
    "    squeezenet.train(False)\n",
    "    squeezenet.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders[TEST]):\n",
    "            if verbose:\n",
    "                if i % 10 == 0:\n",
    "                    print(\"\\rTest batch {}/{}\".format(i, test_batches), end='', flush=True)\n",
    "\n",
    "\n",
    "            inputs, labels = data\n",
    "            if use_gpu:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            else:\n",
    "                inputs, labels = inputs, labels\n",
    "\n",
    "            outputs = squeezenet(inputs)\n",
    "\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss_test += loss.data\n",
    "            acc_test += torch.sum(preds == labels.data).item()\n",
    "\n",
    "            # del inputs, labels, outputs, preds\n",
    "            # torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = loss_test / dataset_sizes[TEST]\n",
    "    avg_acc = acc_test / dataset_sizes[TEST]\n",
    "\n",
    "    elapsed_time = time.time() - since\n",
    "    if verbose:\n",
    "        print()\n",
    "        print(\"Evaluation completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n",
    "        print(\"Avg loss (test): {:.4f}\".format(avg_loss))\n",
    "        print(\"Avg acc (test): {:.4f}\".format(avg_acc))\n",
    "        print('-' * 10)\n",
    "    return avg_acc, avg_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark =True\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "#parser = argparse.ArgumentParser(description='PyTorch SVHN Example')\n",
    "# parser.add_argument('--type', default='cifar10', help='|'.join(selector.known_models))\n",
    "# parser.add_argument('--quant_method', default='linear', help='linear|minmax|log|tanh')\n",
    "#parser.add_argument('--batch_size', type=int, default=100, help='input batch size for training (default: 64)')\n",
    "#parser.add_argument('--gpu', default=None, help='index of gpus to use')\n",
    "#parser.add_argument('--ngpu', type=int, default=8, help='number of gpus to use')\n",
    "#parser.add_argument('--seed', type=int, default=117, help='random seed (default: 1)')\n",
    "# parser.add_argument('--model_root', default='~/.torch/models/', help='folder to save the model')\n",
    "# parser.add_argument('--data_root', default='/tmp/public_dataset/pytorch/', help='folder to save the model')\n",
    "# parser.add_argument('--logdir', default='log/default', help='folder to save to the log')\n",
    "\n",
    "#parser.add_argument('--input_size', type=int, default=224, help='input size of image')\n",
    "# parser.add_argument('--n_sample', type=int, default=20, help='number of samples to infer the scaling factor')\n",
    "#parser.add_argument('--param_bits', type=int, default=8, help='bit-width for parameters')\n",
    "#parser.add_argument('--bn_bits', type=int, default=32, help='bit-width for running mean and std')\n",
    "#parser.add_argument('--fwd_bits', type=int, default=8, help='bit-width for layer output')\n",
    "# parser.add_argument('--overflow_rate', type=float, default=0.0, help='overflow rate')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "args = {}\n",
    "\n",
    "args['batch_size'] = 100\n",
    "args['gpu'] = None\n",
    "args['ngpu'] = 8\n",
    "args['seed'] = 117\n",
    "args['input_size'] = 224\n",
    "#args['param_bits'] = 2\n",
    "#args['bn_bits'] = 2\n",
    "#args['fwd_bits'] = 2\n",
    "args['overflow_rate'] = 0.0\n",
    "\n",
    "#args.gpu = misc.auto_select_gpu(utility_bound=0, num_gpu=args.ngpu, selected_gpus=args.gpu)\n",
    "#args.ngpu = len(args.gpu)\n",
    "#misc.ensure_dir(args.logdir)\n",
    "#args.model_root = misc.expand_user(args.model_root)\n",
    "#args.data_root = misc.expand_user(args.data_root)\n",
    "#args.input_size = 299 if 'inception' in args.type else args.input_size\n",
    "#assert args.quant_method in ['linear', 'minmax', 'log', 'tanh']\n",
    "#print(\"=================FLAGS==================\")\n",
    "#for k, v in args.__dict__.items():\n",
    "#    print('{}: {}'.format(k, v))\n",
    "#print(\"========================================\")\n",
    "\n",
    "#assert torch.cuda.is_available(), 'no cuda'\n",
    "torch.manual_seed(args['seed'])\n",
    "torch.cuda.manual_seed(args['seed'])\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu:\n",
    "    print(\"Using CUDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model():\n",
    "    # load model and dataset fetcher\n",
    "    # model_raw, ds_fetcher, is_imagenet = selector.select(args.type, model_root=args.model_root)\n",
    "    #squeezenet1_1 = models.squeezenet1_1()\n",
    "\n",
    "    # importing fixed version of squeezenet class and functions\n",
    "    import squeezenet_fix\n",
    "\n",
    "    squeezenet1_1 = squeezenet_fix.squeezenet1_1()\n",
    "\n",
    "    # Freeze training for all layers\n",
    "    for param in squeezenet1_1.features.parameters():\n",
    "        param.require_grad = False\n",
    "\n",
    "    # Newly created modules have require_grad=True by default\n",
    "    num_features = squeezenet1_1.classifier[1].in_channels\n",
    "    features = list(squeezenet1_1.classifier.children())[:-3] # Remove last 3 layers\n",
    "    features.extend([nn.Conv2d(num_features, 2, kernel_size=1)]) # Add\n",
    "    features.extend([nn.ReLU(inplace=True)]) # Add\n",
    "    features.extend([nn.AdaptiveAvgPool2d(output_size=(1,1))]) # Add our layer with 2 outputs\n",
    "    squeezenet1_1.classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "\n",
    "    squeezenet1_1.load_state_dict(torch.load('./weights/squeezenet_v1-flower-or-crops.pt'))\n",
    "    #print(squeezenet1_1)\n",
    "    return squeezenet1_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_state_dict(state_dict, bn_bits, param_bits):\n",
    "    if param_bits < 32:\n",
    "        state_dict_quant = OrderedDict()\n",
    "        sf_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            if 'running' in k:\n",
    "                if bn_bits >=32:\n",
    "                    print(\"Ignoring {}\".format(k))\n",
    "                    state_dict_quant[k] = v\n",
    "                    continue\n",
    "                else:\n",
    "                    bits = bn_bits\n",
    "            else:\n",
    "                bits = param_bits\n",
    "\n",
    "    #        if args.quant_method == 'linear':\n",
    "    #            sf = bits - 1. - quant.compute_integral_part(v, overflow_rate=args.overflow_rate)\n",
    "    #            v_quant  = quant.linear_quantize(v, sf, bits=bits)\n",
    "    #        elif args.quant_method == 'log':\n",
    "    #            v_quant = quant.log_minmax_quantize(v, bits=bits)\n",
    "    #        elif args.quant_method == 'minmax':\n",
    "    #            v_quant = quant.min_max_quantize(v, bits=bits)\n",
    "    #        else:\n",
    "    #            v_quant = quant.tanh_quantize(v, bits=bits)\n",
    "            # using uniform affine quantization\n",
    "    #        layer_max = torch.max(v).item()\n",
    "    #        layer_min = torch.min(v).item()\n",
    "            sf = bits - 1. - compute_integral_part(v, overflow_rate=args['overflow_rate'])\n",
    "            v_quant  = linear_quantize(v, sf, bits=bits)     \n",
    "            state_dict_quant[k] = v_quant\n",
    "        return state_dict_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "\n",
    "\n",
    "def quantize_model_forward_activation(model, fwd_bits):\n",
    "    # Quantize the forward activaton of parameters on the model\n",
    "    if fwd_bits < 32:\n",
    "        model = duplicate_model_with_quant(model, bits=fwd_bits)\n",
    "        #print(squeezenet1_1)\n",
    "        #val_ds_tmp = ds_fetcher(10, data_root=args.data_root, train=False, input_size=args.input_size)\n",
    "        #misc.eval_model(model_raw, val_ds_tmp, ngpu=1, n_sample=args.n_sample, is_imagenet=is_imagenet)\n",
    "        if use_gpu:\n",
    "            model.cuda() #.cuda() will move everything to the GPU side\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Tests 31/31"
     ]
    }
   ],
   "source": [
    "bits_list = list(range(1, 32))\n",
    "\n",
    "test_accuracies_list = []\n",
    "test_losses_list = []\n",
    "\n",
    "for idx, bits_item in enumerate(bits_list):\n",
    "    print(\"\\rRunning Tests {}/{}\".format(idx+1, len(bits_list)), end='', flush=True)\n",
    "    # Quantize weights\n",
    "    squeezenet1_1 = create_model()\n",
    "    state_dict = squeezenet1_1.state_dict()\n",
    "    state_dict_quant = quantize_state_dict(state_dict, bits_item, bits_item)\n",
    "    #print(state_dict_quant)\n",
    "    squeezenet1_1.load_state_dict(state_dict_quant)\n",
    "\n",
    "    # Quantize forward activation\n",
    "    squeezenet1_1_quant = quantize_model_forward_activation(squeezenet1_1, bits_item)\n",
    "\n",
    "    # evaluate\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    #exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "    avg_acc, avg_loss = eval_model(squeezenet1_1_quant, criterion)\n",
    "    test_accuracies_list.append(avg_acc)\n",
    "    test_losses_list.append(avg_loss)\n",
    "    \n",
    "    #print()\n",
    "    #print(idx)\n",
    "    #print(bits_item)\n",
    "    #print(avg_acc)\n",
    "    #print(test_accuracies_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8HPV9//HXR7JkSZYt+UIY22BzxmCu4EDSUCJzpM4BhP4IgZQEcjlpSxMoSSFpmxh65A5NC01qCEkTAoZADqDEQAKuQ3MA5rLBHA42+JBtydiy1ta10uf3x3dWXq0lay3tamfX7+fjoYd2Zr47+/nuzO5n5/ud+Y65OyIiIgBlhQ5ARETiQ0lBRET6KCmIiEgfJQUREemjpCAiIn2UFEREpI+SgsSOBd83s+1m9nieXmOdmZ0dPf6Cmd2StuwCM1tvZgkzO9nMjjGzp82szcw+nY94SkXme5nD9X7XzP4xD+tdZGa35Xq9Ga/RaGYb8vkauTSm0AHEmZktA04EDnb3zgKHcyA5HTgHmOHuu/L9Yu7+rxmzvgFc4e6/ADCz7wHL3P3kfMeSycwagdvcfcZov/ZQBoptgPdyOOu9HPi4u5+ett5PjXS9kh0dKQzCzGYBfwo4cN4ov3ZRJes8xHsYsG44CSFHsRwGPL+P6dGOR2T0uLv+BvgDvgj8H/At4P6MZdXAN4HXgFbgMaA6WnY68FtgB7AeuDyav4zw6ye1jsuBx9KmHfhr4BVgbTTv29E6dgIrgD9NK18OfAH4I9AWLZ8J3AR8MyPe+4ArB6nnccDDwBvAFuAL0fwfAP+cVq4R2JA2vQ64BngO6AT+Abg7Y93fBv49elwHfA9oAjYC/wyUDxDPx4AOoAdIANdF8z8BrInivBc4ZF/v3QDr/VC0vbYBfx/Ff3a0bBFwGzA2ek0HdkXv7SNRLB3RsqOjct8AXo/es++mbf9GYEP03mwGfhTNfy/wTLRf/BY4IeO9/Gz0XrYCdwJVwDigHeiNXjuRXu+050+O3pOdwOPAPxHtW8CsqD5j0sovI9oXgSOiOm4DWoAfA/XDjS31XkbPvTFtWQJIAouiZdeyZ999Abggmj8nY/vvGGR/HGp/+BRhf9hO+EzYIPtFX7zR9FvZ8/l9FmiM5l8MPJnx3KuAe6PHQ+4Tac+7hvAZaANeAs4q9Pddv3oVOoC4/kU73F8BpwDdQEPaspuiD9Z0wpfzn0Q7xaHRhr4EqIg+rCdFz+n7IEbTl7N3UngYmJS2M10arWMMcDXhS6YqWvY5YCVwDGCEZq7JwKnAJqAsKjcF2J0ef9prjid8SV9N+KCPB06LlmV+CDN37HWEL7mZhCR5WPQ6E6Ll5dG63xpN/xz4L8KXyUGEL69PDvLeZ743ZxK+sN4cvc//ASzf13uXsb5jCV8wZ0TP/xbhC6pfUshY35Fp05nb7t8IX0STovfsPuDLae9TEvhq9FrVUdxbgdOi9+Wy6P0bm/ZePk74Up0ErAY+NdD7Psj7tQS4K3pv5xK+cLJNCkcSmurGAlOB5cC/ZWznrGPLfC/T5p8ENAMnR9Pvj9ZZBnyAkISnDbT9M/fHLPeH+4F6wmeyGVgwyHvXFy/h87wNeHcU1znR9FSghvDZPirtuU8AF2e5T2yIHh9D+KF3SNr2OaLQ33f93pNCBxDHP8Kv/W5gSjT9InBV9LiM8AvpxAGe93ngZ4Oss++DGE332/GjHfnMIeLannpdwi+M8wcptxo4J3p8BfDAIOUuAZ4eZFnfhzCa7vcFQPiy+GjGcx4DPhw9Pgf4Y/S4gXA0UZ3x2o8O8tqZ7833gK+lTddG22dWNu8d4ahvSdr0OKCLYSQFQgLelf5BBt7GnqO7xmjdVWnLvwP8U0ZMLwHvSHsvL01b9jXguwO97wPUrTx6L96UNu9fyTIpDLC+96XvE/sbW+Z7Gc2bGq3n4n3U4xmi/Tlz+2fuj1nuD6enLb8LuHaQ1+2Ll/AL/kcZyx8ELose3wZ8MXp8FCFJ1GS5T6SSwpGEHwhnAxWDvR+F/FOfwsAuAx5y95Zo+vZoHoRf3lWEQ99MMweZn6316RNmdrWZrTazVjPbQWiCmZLFa/034SiD6P+PBimX03gJ79Ml0eMPRtMQjiIqgCYz2xHV5b8IRwzZOITQ9AOAuycIv+Cm7yOWzOf3LffQV7Ety9fOlPrVuCKtLkuj+SnN7t6RNn0YcHWqfPScmVFcKZvTHu8mfNFlG88Y+tf/tUHK7sXMDjKzJWa20cx2Er74pmQUG25smFkFcDdwu7svSZv/YTN7Ju39mDvA6w4mm/1hODEfBrw/YzudDkyLlmfu3z93991kt0+kYl0DXElIRluj9/6QzHKFpKSQwcyqgYuAd5jZZjPbTGg7PNHMTiQctnYQ2mIzrR9kPoRfEjVp0wcPUMbT4vhTwi+Xi4CJ7l5PaNO1LF7rNuD8KN45hKabgeQs3shPgEYzmwFcwJ6ksJ5wpDDF3eujvwnuftwgr51pE+EDC4CZjSM0lW3cRyzpmghfwqnn10TPH44WwpHicWl1qXP39C+dzFjWA/+SVr7e3Wvc/Y4sXm9f9YLQNJIkrX6EJpOUVGf9YNvyy9FrnODuEwg/IozsDBUbhKadNkKfEwBmdhhwM+EodnK0b69Ke92h1pvN/jAc6wlHCunbaZy7fyVa/hAwxcxOIiSH1P6dzT7Rx91v93Bm1WGEun51hHHnlJLC3t5H6OQ6ltAOehLhi/U3hKaRXuBW4FtmdoiZlZvZ28xsLKGT7mwzu8jMxpjZ5GgHgnB4/OdmVmNmRxI6VPdlPOHD3gyMMbMvAhPSlt8C/JOZHRWd13+CmU0GcPcNhPbOHwH3uHv7IK9xP3CwmV1pZmPNbLyZnZYW77vNbJKZHUz4dbNP7t5MaJr4PuHQeXU0v4nwgfqmmU0wszIzO8LM3jHUOiO3Ax8xs5Oi9/lfgT+4+7osn3838F4zO93MKoHrGea+H23/m4EbzOwgADObbmZ/to+n3Qx8ysxOi7bVODN7j5mNz+IltwCTzaxukHh6gJ8Ci6J961j2HNWmtslG4NJoX/0o/X8IjCfq0DWz6YS+qmztMzYz+yTwDuCD0fuWMo7wZdgclfsI4Ughfb0zom01kJHuD4O5DTjXzP4seq+qomsMZgC4e5KwL32d0HfwcDQ/630iuublzCjuDkIy6Rlh3DmlpLC3y4Dvu/vr7r459Uc4k+IvolMMP0vo5H2CcPbDVwkdu68TOqmujuY/Q+gABriB0Na8hdC88+Mh4ngQ+CXwMuFQuYP+TQTfIrSVPkQ46+R7hE7NlP8GjmfwpiPcvY3Q9n8u4XD7FWB+tPhHhLMv1kWvcecQ8abcTmgvvT1j/oeBSsKZJtsJH65pZMHdfw38I3AP4Vf/EYSzQbLi7s8Tzk66PXr+dsIZQsN1DeFEhN9HTS6/InQgDvb6TxLOlrkxeu01hHbzbGJ/EbgDeDVqmhioqeEKQvPIZkLb+/czln+C8GW/jXC22W/Tll1H6LBtBf6HkGCykkVslwCHA5ssXAiYMLMvuPsLhLP3fkf4PBxPONMv5RHCKcCbzawlY50j3h/2UZ/1wPmEs/qaCZ+3z9H/ezK1f/8kShIp2e4TY4GvEI4uNhOaUL8w0thzyaLODykxZnYG4ZfPrIxfaVLiBrr4SyRbOlIoQVHn3meAW5QQRGR/KCmUGDObQ7jwZhrh3GkRkayp+UhERProSEFERPrkbbAuM7uVMN7LVnefO8ByI4yN827CxSWXu/tTQ613ypQpPmvWrH7zdu3axbhx43IRdsGpLvFTKvUA1SWuRqMuK1asaHH3vS6o20u+LpUmjDPzZmDVIMvfTTjl0giDUP0hm/WecsopnunRRx/da16xUl3ip1Tq4a66xNVo1IWMAf0G+8tb85G7Lyecqz+Y84EfRvH+Hqg3s6zOWxcRkfzIa0ezhXsS3O8DNx/dD3zF3R+Lpn8NXOPhQp/MsguBhQANDQ2nLFmypN/yRCJBbW3Ww7HEmuoSP6VSD1Bd4mo06jJ//vwV7j5vqHKFvAHIQOOrDJih3H0xsBhg3rx53tjY2G/5smXLyJxXrFSX+CmVeoDqEldxqkshzz7aQP9BvGYQBroSEZECKWRSuBf4cDRA2FuBVg8Dp4mISIHk85TUOwg3l5hiZhuALxHG1Mfdvws8QDgDaQ3hlNSP5CuWA9HqplaWrtrCxh3tTK+vZsHcBuZMG3AwSxGRPnlLCu5+yRDLnTBypeTY6qZWFi9fS111BdPqqmht72bx8rUsPGP2Xokh2+SxP0kmVXbFi5083f3yiNdZ6HK5qkcp1SWf+02x16WQn5VcKF+0aFFeVpwvixcvXrRw4cJ+89atW0fmBW3FKhd1ue33r+PubN/dxR+bd9He3UN3spc/Nu/izYdOpKqiDDPrSx4Ak2sr2dmR5Ld/3MZhk6uZOr6qb33ZlsssS8dOyqpqR7TOOJTLRT1KqS753m+KuS6F/KwM5brrrmtatGjR4qHKFd3YR/PmzfMnn+x/1mqceu5HaqR12dzawUd/8ASvbUuwq2vgAVIry8s4aMJYenqdynKjvqaSmspyysqMrmQv4yrHcM5xDVSUl1FRbjz0/GY6unuoHVtBWZnR2+skOrsZW1HOO489mGSP093bS7LH+fXqLezqSlJZXsYb23cwfkId7d1JxpSXcfz0ulC2x1m1cQedyV7KbM9JaN09vYwpL+PIg/acmrdma4JkTy8V5WX7Va68zEj29lJVUc5JM+upKC9jTFkZz27YTneyl+rKMRjQ687urh4qyss4cWY9XT29JHt6eW59K509PZSZsWvXbmpqakYU30jqkstyu3ePvC65LldKdRlunbOty/Ez6pg1eRyt7d3UVVdw1TlHky0zi/0pqTIMAx1GHtMwgeWvNPPjP7zOIy9upafXOaRuLI3HTOTQyePo6Ophy85wy+B5syaxpa2D5p2dPLamhdauJE2tHXT19P9xsOzl5qziWbpqyz6X2/btmEGZGevfaGdMmVFRXsaO9i4qyozysjJSecHdSfbChu17bhT3xq4uxpSBpSWPocq5Q487PT29JHuhJdFFsqeX7h5nd1eS3ozfQQaUlRmvtuyiotwYU1bGzo7uEF+5kUw6yY7uEcU33LrkulxXDuqS63KlVJfh1jnbunQnww+98VVj2LhjsBsqjoySQhHJ7CvYvLOdq3/yHC1tHWxt62LyuEoWnnE4bzlsIvc910RddQVjygwHxlaU79WncMPDL/f94uh1p7c3NDnVVo3hY6cfTndP+PX/vcfW0treTU1lOb3ulJnR3tXDhOoKPnb6bMaUW/RL3Lj5N6+S6EhSX1PJ+tfXMWvW7AF/1aS/dspolptQFXZ9MxtyfaFJ77ARvW4c6pyruuS6XCnVZbh13t+6tHUkmV5fTT5olNQisnTVFuqqK0j29vLLVU3c9cQGXti0k8rycm784Mn87vNncc2CN3HmnAYWnjGbuuoKmlo7qKuuGLCTecHcBlrbu2lt7wZgV1cP7d29XHDydBomVDFjYg2zpozjg6fNpLzMqKoop2FCFVUVoanpg6fNZNaUccyYWEPDhCom147l/JMOYVdXD4nOJO70rX/B3IZBX7vXfdTL7exI4mQXn+fgdeNQ51zVJdflSqkuw61zrvaxXFBHc8zsqy73PLWBybWV/OypjTS3dXLizHrOmnMQB9dVc8WZR1FetueQder4Kt52xGTeedzBvO2IyQN2SE0dX8Vhk6vZsL2dTa0dTB0/lg+8ZcZeySPbcpllX92yg9nTJo9onXEol4t6lFJd8r3fFHNdCvlZGYo6movUvupyw8Mv81rLLn7+7CbmHzOVE2bUD3poGgelsl1KpR6gusTVaNQl245mNR8VkQVzG1i9uQ2Aw6eOy/thpIgceNTRXETedPAEWtu7OHRSNa3toaNpOIeRIiKDUVIoIis3trJ5Zydfu/AELpo3c+gniIjsJzUfFZH7nt1ERbnxZ8cdXOhQRKREKSkUid5e5/7nmnjH0Qf1O19ZRCSXlBSKxJOvbaeptYNzT9QdS0Ukf5QUisR9z26iqqKMs+foTCMRyR8lhSKQ7OnlgZVNnDWngXFjdW6AiOSPkkIR+N2r29i2q4tzTzik0KGISIlTUigC9z27idqxY2g8ZmqhQxGREqekEHOdyR6WrtrMO49roKqivNDhiEiJU1KIud+83MLOjiTnnqimIxHJPyWFmLvvuU3U11Rw+pFTCh2KiBwAlBRirL2rh4df2MK75k7rd3s/EZF80TdNjD3y4lZ2d/XogjURGTVKCjF237ObmDp+LKfNnlzoUETkAKGkEFNtHd088tJW3nP8tH53VBMRySclhZh6+IUtdCV7ddaRiIwqJYWYuvfZTUyvr+bNh9YXOhQROYAoKcTQG7u6eOyVFs498RDM1HQkIqNHSSGGlq7aTLLXddaRiIw6JYUYuu/ZTRw+dRzHTptQ6FBE5ACjpBAzOzp6+f3abZx7gpqORGT0KSnEzBObe3BHTUciUhBKCjHzh81J5kybwJEHjS90KCJyAMprUjCzBWb2kpmtMbNrB1h+mJn92syeM7NlZjYjn/HE2eqmVq6793nW7OhlYk0Fq5taCx2SiByA8pYUzKwcuAl4F3AscImZHZtR7BvAD939BOB64Mv5iifOVje1snj5Wp7buAOAQ+qqWLx8rRKDiIy6fB4pnAqscfdX3b0LWAKcn1HmWODX0eNHB1h+QFi6agt11RWsf6Od+rEwfWINddUVLF21pdChicgBJp93gZ8OrE+b3gCcllHmWeD/Ad8GLgDGm9lkd9+WXsjMFgILARoaGli2bFm/lSQSib3mFZMVL3YyaSy8kUgyfZyzbt063J01nbCsYlOhwxu2Yt8uKaVSD1Bd4ipOdclnUhjofErPmP4scKOZXQ4sBzYCyb2e5L4YWAwwb948b2xs7Ld82bJlZM4rJk93v8wbuzpJvvo6NZXlzJo1i9b2bmZXV9DYeHShwxu2Yt8uKaVSD1Bd4ipOdcln89EGYGba9Ayg389ed9/k7n/u7icDfx/NO+Aa0hfMbWBrWycAlWVOa3s3re3dLJjbUODIRORAk8+k8ARwlJnNNrNK4GLg3vQCZjbFzFIxfB64NY/xxNacaXUsOC4kgO5eo666goVnzGbOtLoCRyYiB5q8JQV3TwJXAA8Cq4G73P15M7vezM6LijUCL5nZy0AD8C/5iifuaqsqAXj/MZVcdc7RSggiUhD57FPA3R8AHsiY98W0x3cDd+czhmLRkgjNRxMqNbSFiBSOrmiOiZaoT2HCWCUFESkcJYWYaEl0MqFqDBW69aaIFJCSQkw0JzqZOn5socMQkQOckkJMtLR1MaVWSUFECktJISZaEp1M0ZGCiBSYkkJMNCc6maojBREpMCWFGOjo7qGtI8mU2spChyIiBzglhRhIXaOgjmYRKTQlhRhoSXQBqKNZRApOSSEGUheuKSmISKEpKcRAc9R8pLOPRKTQlBRiYM+RgjqaRaSwlBRiIDXExdgx5YUORUQOcEoKMdCS6FLTkYjEgpJCDDQnOtXJLCKxoKQQAy1tuppZROJBSSEGNEKqiMSFkkKBaYgLEYkTJYUC27ZLVzOLSHwoKRRYs65mFpEYUVIosNSFa+pTEJE4UFIosBYNcSEiMaKkUGCppDB5nDqaRaTwlBQKrCXRxfiqMVRVaIgLESk8JYUCa9aFayISI0oKBdac6FR/gojEhpJCgbUkdKQgIvGhpFBgLW2duppZRGJDSaGAOrp72NmR1IVrIhIbSgoFlBriQheuiUhcKCkUUIuGuBCRmFFSKCBdzSwicZPXpGBmC8zsJTNbY2bXDrD8UDN71MyeNrPnzOzd+YwnbvYMhqeOZhGJh7wlBTMrB24C3gUcC1xiZsdmFPsH4C53Pxm4GPjPfMUTR31HCmo+EpGYyOeRwqnAGnd/1d27gCXA+RllHJgQPa4DNuUxntjREBciEjfm7vlZsdmFwAJ3/3g0/SHgNHe/Iq3MNOAhYCIwDjjb3VcMsK6FwEKAhoaGU5YsWdJveSKRoLa2Ni/1yKf/fKaD13f28pUzavrmFWtdBlIqdSmVeoDqElejUZf58+evcPd5Q5Ubk8cYbIB5mRnoEuAH7v5NM3sb8CMzm+vuvf2e5L4YWAwwb948b2xs7LeSZcuWkTmvGHznpd9xaBU0Nr6tb16x1mUgpVKXUqkHqC5xFae65LP5aAMwM216Bns3D30MuAvA3X8HVAFT8hhTrIRxj9TJLCLxkc+k8ARwlJnNNrNKQkfyvRllXgfOAjCzOYSk0JzHmGKlRSOkikjMZJUUzOweM3uPmWWdRNw9CVwBPAisJpxl9LyZXW9m50XFrgY+YWbPAncAl3u+OjlipjOpIS5EJH6y7VP4DvAR4N/N7CeEfoAXh3qSuz8APJAx74tpj18A3p59uKVjWyIMcaEL10QkTrL65e/uv3L3vwDeDKwDHjaz35rZR8ysIp8BlqpmDXEhIjGUdXOQmU0GLgc+DjwNfJuQJB7OS2Qlbs+Fa+poFpH4yKr5yMx+CrwJ+BFwrrs3RYvuNLMn8xVcKUslBY2QKiJxkm2fwo3u/shAC7K5GEL21pLqU1DzkYjESLbNR3PMrD41YWYTzeyv8hTTAaG5rZPxYzXEhYjES7ZJ4RPuviM14e7bgU/kJ6QDQ7hwTUcJIhIv2SaFMjPrG7YiGgFVPaQjoAvXRCSOsk0KDwJ3mdlZZnYm4UKzpfkLq/S1aIgLEYmhbDuarwE+CfwlYaC7h4Bb8hXUgaAl0cXbdaQgIjGTVVKIRi39TvQnI9SZ7KG1vVtnHolI7GR7ncJRwJcJd1CrSs1398PzFFdJ26bTUUUkprLtU/g+4SghCcwHfki4kE2GQReuiUhcZZsUqt3914Q7tb3m7ouAM/MXVmnTEBciElfZdjR3RMNmv2JmVwAbgYPyF1Zp02B4IhJX2R4pXAnUAJ8GTgEuBS7LV1ClLjXEhZqPRCRuhjxSiC5Uu8jdPwckCPdVkBFobuukVkNciEgMDXmk4O49wCnpVzTLyLQkOnWUICKxlG2fwtPAL6K7ru1KzXT3n+YlqhLXkuhUJ7OIxFK2SWESsI3+Zxw5oKQwDM1tnRzdML7QYYiI7CXbK5rVj5BDLYku/uQINR+JSPxke0Xz9wlHBv24+0dzHlGJ60r20trerT4FEYmlbJuP7k97XAVcAGzKfTilb9suXaMgIvGVbfPRPenTZnYH8Ku8RFTi9ly4po5mEYmfbC9ey3QUcGguAzlQ9A1xoeYjEYmhbPsU2ujfp7CZcI8F2U8tbdHVzGo+EpEYyrb5SOdP5khzQn0KIhJfWTUfmdkFZlaXNl1vZu/LX1ilqyURhriortQQFyISP9n2KXzJ3VtTE+6+A/hSfkIqbc1tuppZROIr26QwULlsT2eVNGGICzUdiUg8ZZsUnjSzb5nZEWZ2uJndAKzIZ2ClqiXRpQvXRCS2sk0KfwN0AXcCdwHtwF/nK6hSpiMFEYmzbM8+2gVcm+dYSl5Xspcdu7uVFEQktrI9++hhM6tPm55oZg9m8bwFZvaSma0xs72SipndYGbPRH8vm9mO/Qu/uPQNcTFeHc0iEk/ZdhZPic44AsDdt5vZPu/RHN2x7SbgHGAD8ISZ3evuL6St56q08n8DnLw/wReb1IVrOlIQkbjKtk+h18z6hrUws1kMMGpqhlOBNe7+qrt3AUuA8/dR/hLgjizjKUqpIS7U0SwicWXuQ323h2YgYDHwv9GsM4CF7j5oE5KZXQgscPePR9MfAk5z9ysGKHsY8HtgRnT7z8zlC4GFAA0NDacsWbKk3/JEIkFtbe2Q9Si05Ru6uXVVF18/o5qpNQPn42KpSzZKpS6lUg9QXeJqNOoyf/78Fe4+b6hy2XY0LzWzeYQv5meAXxDOQNqXge7pPFgGuhi4e6CEEL3+YkJSYt68ed7Y2Nhv+bJly8icF0fPP7oGVr3Ee89+x6BXNBdLXbJRKnUplXqA6hJXcapLtgPifRz4DDCDkBTeCvyO/rfnzLQBmJk2PYPB78FwMQfAKa4tiU7GVZZriAsRia1s+xQ+A7wFeM3d5xM6hJuHeM4TwFFmNtvMKglf/PdmFjKzY4CJhCRT0nThmojEXbZJocPdOwDMbKy7vwgcs68nuHsSuAJ4EFgN3OXuz5vZ9WZ2XlrRS4Alnk3nRpFradOFayISb9mekrohuk7h58DDZradLG7H6e4PAA9kzPtixvSiLGMoes2JTo6cWhodYyJSmrLtaL4gerjIzB4F6oCleYuqRLUkOnnr4ZMKHYaIyKD2e6RTd//foUtJpu4eDXEhIvE33Hs0y37alohuw6mOZhGJMSWFUdLcpttwikj8KSmMkhbdm1lEioCSwihpTo17pKQgIjGmpDBK+o4UNGy2iMSYksIoaW4LQ1zUVOrW1iISX0oKo6Ql0cUUnXkkIjGnpDBKNMSFiBQDJYVR0pLoZEqt+hNEJN6UFEZJS6JTF66JSOwpKYyC7p5etmuICxEpAkoKoyA1xIWSgojEnZLCKNDVzCJSLJQURkHf1czqUxCRmFNSGAWpwfA0xIWIxJ2SwijQEBciUiyUFEZBS1sXNRriQkSKgJLCKAgXrqnpSETiT0lhFOjCNREpFkoKeba6qZUXm3bS1NrODQ+/zOqm1kKHJCIyKCWFPFrd1Mri5WtJdCaZVFNBa3s3i5evVWIQkdhSUsijpau2MH5sOV09zrixFdRVh7+lq7YUOjQRkQHpdJg82rijnTILjydHI6SOrxrDxh3tBYxKRGRwOlLIo+n11by+bTcADeOrAGjrSDK9vrqQYYmIDEpJIY8WzG1g884OKsuN2qpyWtu7aW3vZsHchkKHJiIyIDUf5dGcaXWUGRxcV0VTayfT66v5wFtmMGdaXaFDExEZkJJCHnV09/D6G+184ozDuWbBmwodjojIkNR8lEcvbW4j2escP11HBiJSHJQU8mjlxnA9gpKCiBQLJYU8WrmhlfqaCmZM1NlGIlIc8poUzGyBmb1kZmvM7NpBylxkZi+Y2fNmdns+4xltKze2cvz0Osys0KGIiGQlb0mY9C7oAAAN+UlEQVTBzMqBm4B3AccCl5jZsRlljgI+D7zd3Y8DrsxXPKOto7uHl7e0qelIRIpKPo8UTgXWuPur7t4FLAHOzyjzCeAmd98O4O5b8xjPqHpRncwiUoTM3fOzYrMLgQXu/vFo+kPAae5+RVqZnwMvA28HyoFF7r50gHUtBBYCNDQ0nLJkyZJ+yxOJBLW1tXmpx3A98no3P3yhi6+fUc3UmuxzbxzrMlylUpdSqQeoLnE1GnWZP3/+CnefN1S5fF6nMFBDemYGGgMcBTQCM4DfmNlcd9/R70nui4HFAPPmzfPGxsZ+K1m2bBmZ8wrtl3c/x8SazVz4rvn71acQx7oMV6nUpVTqAapLXMWpLvlsPtoAzEybngFsGqDML9y9293XAi8RkkTRe25jK3PVySwiRSafSeEJ4Cgzm21mlcDFwL0ZZX4OzAcwsynA0cCreYxpVHR09/CKOplFpAjlLSm4exK4AngQWA3c5e7Pm9n1ZnZeVOxBYJuZvQA8CnzO3bflK6bRkupkPmGGkoKIFJe8jn3k7g8AD2TM+2LaYwf+NvorGSs3hC6RuTpSEJEioyua82DlxlYm1lTovgkiUnSUFPJg5cadHD+jXp3MIlJ0lBRybM+VzBMKHYqIyH5TUsix1U076dGVzCJSpJQUcmxVarjsGfUFjkREZP8pKeTYcxtamTSukkPqqgodiojIflNSyLGVupJZRIqYkkIOdXT38MrWhDqZRaRoKSnk0At9nczqTxCR4qSkkEN7Opl15pGIFCclhRxaqU5mESlySgo5pHsyi0ixU1LIkT2dzGo6EpHipaSQI6lOZo2MKiLFTEkhR1ZuCJ3MuoeCiBQzJYUcWbmxlcnjKpmmTmYRKWJKCjmySlcyi0gJUFLIgfauMFy2mo5EpNgpKeTAC0076XXdflNEip+SQg70XcmspCAiRU5JIQee29DKlFp1MotI8VNSyAF1MotIqVBSGKH2rh5e2dqmpiMRKQlKCiP0QlMrva7+BBEpDUoKI5S6klnDZYtIKVBSGKGVG3cypbaSgyeok1lEip+Swgit3LhDw2WLSMlQUhiB3V1J1mi4bBEpIUoKI7BaVzKLSIlRUhiB5/qGy64vcCQiIrmhpDBMq5taufOJ9YwdU8Ydj7/G6qbWQockIjJiSgrDsLqplcXL19LU2s7BE8bS2p5k8fK1SgwiUvTG5HPlZrYA+DZQDtzi7l/JWH458HVgYzTrRne/JddxrG5qZemqLWzc0c70+moWzG1gzrS9+wGyKZfs6eWOx9ezY3cXre1Jjjl4AnXVFQAsXbVlwPWKiBSLvCUFMysHbgLOATYAT5jZve7+QkbRO939inzFkfpVX1ddwbS6Klrbu1m8fC0Lz5jd7ws8s9wbuzr5xoMv85ZZE2nv7mXN1gRrtiZY27KLrp7eUEfg0InVAIyvGsPGHe35qoaIyKjI55HCqcAad38VwMyWAOcDmUkhr5au2kJddQUbtu/mqde2A9DT6/zvS1uZXDu2r9y2RCc9vU55mZHsdXZ2JAH49YtbKTOYOamGow6qpfFNU3l1a4LKMWUcOqmGyjHlALR1JJleXz2aVRMRyTlz9/ys2OxCYIG7fzya/hBwWvpRQdR89GWgGXgZuMrd1w+wroXAQoCGhoZTlixZ0m95IpGgtrZ2wDhuWdnJpLGwebezvi38wnd3unuN2XV7ulTWtvZSUeaYGWUGtRXGuApwjL88cSyV5XsuTnt9Zw9L13VTM8aoGQO7k7A76SyYVcGhE8qH8W5lV5diUyp1KZV6gOoSV6NRl/nz569w93lDlcvnkcJAl/hmZqD7gDvcvdPMPgX8N3DmXk9yXwwsBpg3b543Njb2W75s2TIy56U83f0yre3dzK6u4G3RvNb2buqqK7jqnKP7yt3w8Mt988ko986zjibTW9L6H2bvo59if+2rLsWmVOpSKvUA1SWu4lSXfCaFDcDMtOkZwKb0Au6+LW3yZuCruQ5iwdwGFi9fC4R2/7aOJK3t3XzgLTOGVS5lzrQ6dSqLSMnJ5ympTwBHmdlsM6sELgbuTS9gZtPSJs8DVuc6iDnT6lh4xmzqqitoau2grrpir07m/SknIlLK8nak4O5JM7sCeJBwSuqt7v68mV0PPOnu9wKfNrPzgCTwBnB5PmLJ9le9fv2LyIEur9cpuPsDwAMZ876Y9vjzwOfzGYOIiGRPVzSLiEgfJQUREemjpCAiIn2UFEREpE/ermjOFzNrBl7LmD0FaClAOPmgusRPqdQDVJe4Go26HObuU4cqVHRJYSBm9mQ2l28XA9UlfkqlHqC6xFWc6qLmIxER6aOkICIifUolKSwudAA5pLrET6nUA1SXuIpNXUqiT0FERHKjVI4UREQkB5QURESkT9EnBTNbYGYvmdkaM7u20PGMhJmtM7OVZvaMmT1Z6HiyZWa3mtlWM1uVNm+SmT1sZq9E/ycWMsZsDVKXRWa2Mdouz5jZuwsZYzbMbKaZPWpmq83seTP7TDS/6LbLPupSjNulysweN7Nno7pcF82fbWZ/iLbLndHtBgoTYzH3KZhZOeE2nucQburzBHCJu4/qfaBzxczWAfPcvaguyDGzM4AE8EN3nxvN+xrwhrt/JUrWE939mkLGmY1B6rIISLj7NwoZ2/6I7lUyzd2fMrPxwArgfYTh6Ytqu+yjLhdRfNvFgHHunjCzCuAx4DPA3wI/dfclZvZd4Fl3/04hYiz2I4VTgTXu/qq7dwFLgPMLHNMBx92XE+6Hke58wu1Vif6/b1SDGqZB6lJ03L3J3Z+KHrcRbmA1nSLcLvuoS9HxIBFNVkR/TrgN8d3R/IJul2JPCtOB9WnTGyjSnSXiwENmtsLMFhY6mBFqcPcmCB9q4KACxzNSV5jZc1HzUuybXNKZ2SzgZOAPFPl2yagLFOF2MbNyM3sG2Ao8DPwR2OHuyahIQb/Hij0p2ADzirc9DN7u7m8G3gX8ddSUIYX3HeAI4CSgCfhmYcPJnpnVAvcAV7r7zkLHMxID1KUot4u797j7SYT71p8KzBmo2OhGtUexJ4UNwMy06RnApgLFMmLuvin6vxX4GWGHKVZbUvfgjv5vLXA8w+buW6IPci9wM0WyXaI263uAH7v7T6PZRbldBqpLsW6XFHffASwD3grUm1nqTpgF/R4r9qTwBHBU1HNfCVwM3FvgmIbFzMZFnWiY2TjgncCqfT8r1u4FLoseXwb8ooCxjEjqSzRyAUWwXaIOze8Bq939W2mLim67DFaXIt0uU82sPnpcDZxN6CN5FLgwKlbQ7VLUZx8BRKeh/RtQDtzq7v9S4JCGxcwOJxwdQLh39u3FUhczuwNoJAz/uwX4EvBz4C7gUOB14P3uHvsO3EHq0khoonBgHfDJVLt8XJnZ6cBvgJVAbzT7C4S2+KLaLvuoyyUU33Y5gdCRXE74UX6Xu18fff6XAJOAp4FL3b2zIDEWe1IQEZHcKfbmIxERySElBRER6aOkICIifZQURESkj5KCiIj0UVKQA5qZXWlmNWnTD6TOIy/Eevax7g9Hj5eZ2V43eDez81KjBJvZ+8zs2CHW+d7UCJ0i6XRKqhzQcjUybb5GuI2ucn0KeLO7J81sGfBZdx90aHUz+wFwv7vfvY8yFq337e6+O5cxS3HTkYLElpn9vYV7ZfzKzO4ws89G8/t+LZvZlOgLGTObZWa/MbOnor8/ieY3Rs+528xeNLMfW/Bp4BDgUTN7NCq7Llrnp9LG6V+btvw7ZvZkxlj4g64nevy3ZrYq+rsyLdbVZnZztK6HoitcM50JPJU2WBrApWb222h9p0bru9zMbozqfB7w9Sj2I8zs02b2QjRw3BIIo3UShlh4b042lpQOd9ef/mL3B5xCuIK1BpgArCH8QobwZTYvejwFWBc9rgGqosdHAU9GjxuBVsKYMmXA74DTo2XrgClpr5s5XUG4mvbcaHpS9L88iuOEfa0nrR7jgFrgecIon7OAJHBSVP4uwlWsme/DdcDfpE0vA26OHp8BrIoeXw7cGD3+AXBh2nM2AWOjx/Vp8/8C+I9Cb2v9xetPRwoSV38K/Mzdd3sYETObMa0qgJvNbCXwEyC9Xf1xd9/gYfC0Zwhfytn4NvCIu98XTV9kZk8RhiI4LuM1BnJ6VI9dHsbR/2lUN4C17v5M9HjFIDFNA5oz5t0Bffd+mJBF38VzwI/N7FJCIkrZSjjCEemjpCBxNliHV5I9+25V2vyrCOMVnQjMA9JvaZg+jkwPYXypfTKzy4HDCL/WMbPZwGeBs9z9BOB/Ml5/wNXsY1k2MbUP8BqZ78tQHYPvAW4iHLWsSBuNsypav0gfJQWJq+XABWZWHY0ee27asnWELzjYM7IkQB3QFB0NfIjQxDOUNmB85kwzO4WQAC6N1gehGWsX0GpmDYT7XuxzPVE93mdmNdHotxcQmqOytRo4MmPeB6IYTwda3b11sDqZWRkw090fBf4OqCc0YwEcTRGMLCqjS0lBYsnD7RfvJDT13EP/L9JvAH9pZr8ltNun/CdwmZn9nvCFtyuLl1oM/DLVQZzmCsKIlY9GHba3uPuzhGaj54Fbgf8baj1RPX4APE4YofQWd386i7hSfknoO0i3Par7d4GPDfCcJcDnzOxpQt/KbVGT2tPADR7G8QeYTzjaEemjU1KlKJjZIorsJu25YmY/A/7O3V/J4TobCMOzn5WrdUpp0JGCSPxdS+hwzqVDgatzvE4pATpSEBGRPjpSEBGRPkoKIiLSR0lBRET6KCmIiEgfJQUREenz/wF/XyhEri2pnQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data for plotting\n",
    "t = np.array(bits_list)\n",
    "s = np.array(test_accuracies_list)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t, s)\n",
    "plt.plot(t, s, 'C0o', alpha=0.5)\n",
    "\n",
    "ax.set(xlabel='quantization (bits)', ylabel='accuracy',\n",
    "       title='Accuracy curve for different quantization levels')\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(\"acc_over_bits.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch.optim as optim\n",
    "\n",
    "# eval model\n",
    "#val_ds = ds_fetcher(args.batch_size, data_root=args.data_root, train=False, input_size=args.input_size)\n",
    "#acc1, acc5 = misc.eval_model(squeezenet1_1, val_ds, ngpu=args.ngpu, is_imagenet=is_imagenet)\n",
    "\n",
    "# print sf\n",
    "#print(squeezenet1_1)\n",
    "#res_str = \"type={}, quant_method={}, param_bits={}, bn_bits={}, fwd_bits={}, overflow_rate={}, acc1={:.4f}, acc5={:.4f}\".format(\n",
    "#    args.type, args.quant_method, args.param_bits, args.bn_bits, args.fwd_bits, args.overflow_rate, acc1, acc5)\n",
    "#print(res_str)\n",
    "#with open('acc1_acc5.txt', 'a') as f:\n",
    "#    f.write(res_str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-env] *",
   "language": "python",
   "name": "conda-env-pytorch-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
